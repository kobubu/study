{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMRbqt-Isukm"
      },
      "source": [
        "<h1 style=\"text-align: center;\"><b>Предобработка текста.</b></h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-sGLT0vp4jt"
      },
      "source": [
        "## Часть 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn8EWAjnr18g"
      },
      "source": [
        "### Токенизация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btBdBLxbgrNV",
        "outputId": "fa814771-ae5e-4557-a3cc-e4f736013c6b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "# @title Текст заголовка по умолчанию\n",
        "\n",
        "# Импортируем библиотеку Natural Language Toolkit (NLTK) для работы с текстом\n",
        "import nltk\n",
        "\n",
        "# Скачиваем модель 'punkt_tab', необходимую для токенизации текста\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Подключаем функции для разбиения текста: word_tokenize — на слова, sent_tokenize — на предложения\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eq-QOD9NlO_Q",
        "outputId": "166c9051-7aab-4da3-b48c-c65b14dff6cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['all', 'work', 'and', 'no', 'play', 'makes', 'jack', 'a', 'dull', 'boy', ',', 'all', 'work', 'and', 'no', 'play']\n"
          ]
        }
      ],
      "source": [
        "data = \"All work and no play makes jack a dull boy, all work and no play\"\n",
        "\n",
        "# Преобразуем текст в нижний регистр и разбиваем его на отдельные слова (токены)\n",
        "tokens = word_tokenize(data.lower())\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFDKUzkS6Mci",
        "outputId": "39c615d5-7f99-48d8-ed72-62236b1cd98a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['I was going home when she rung.', 'It was a surprise.']\n"
          ]
        }
      ],
      "source": [
        "# Разбиваем текст на отдельные предложения\n",
        "print(sent_tokenize(\"I was going home when she rung. It was a surprise.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQoG7qznyuf5"
      },
      "source": [
        "[<img src=\"https://raw.githubusercontent.com/natasha/natasha-logos/master/natasha.svg\">](https://github.com/natasha/natasha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPeApu2mwYxY"
      },
      "source": [
        "[Razdel](https://natasha.github.io/razdel/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwJj5Z2fvbeN"
      },
      "outputs": [],
      "source": [
        "# Установка библиотеки razdel для токенизации русского текста\n",
        "!pip install -q razdel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uy58Xd_vve-z",
        "outputId": "cd654248-3a3d-42fe-a1a9-26eca7516e69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Substring(0, 13, 'Кружка-термос'),\n",
              " Substring(14, 16, 'на'),\n",
              " Substring(17, 20, '0.5'),\n",
              " Substring(20, 21, 'л'),\n",
              " Substring(22, 23, '('),\n",
              " Substring(23, 28, '50/64'),\n",
              " Substring(29, 32, 'см³'),\n",
              " Substring(32, 33, ','),\n",
              " Substring(34, 37, '516'),\n",
              " Substring(37, 38, ';'),\n",
              " Substring(38, 41, '...'),\n",
              " Substring(41, 42, ')')]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from razdel import tokenize, sentenize\n",
        "text = 'Кружка-термос на 0.5л (50/64 см³, 516;...)'\n",
        "list(tokenize(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrmhCpNdQo6r"
      },
      "source": [
        "#### Регулярные выражения\n",
        "\n",
        "Исчерпывающий пост https://habr.com/ru/post/349860/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Опять же используем регулярные выражения для предобработки текста от лишних символов"
      ],
      "metadata": {
        "id": "oLCumSc1uNmI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IccRpcG06Mfd",
        "outputId": "95f519c3-34f7-4c71-c5f9-582bc11359f1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['super', 'c', 'a', 'a', 'c', 'a', 'c']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "word = 'supercalifragilisticexpialidocious'\n",
        "re.findall('[abc]|up|super', word)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rVn-TQDiuKWy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Je8YPHLZPJW7",
        "outputId": "c60f1762-5bc3-4f34-c6e4-47f88f336f6d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['49', '432', '312']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "re.findall('\\d{1,3}', 'These are some numbers: 49 and 432312')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fzde8MX1PJXA",
        "outputId": "ac47f2f8-22a4-4a8a-bd7c-7377604c9637"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'How to split text'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "re.sub('[,\\.?!]','','How, to? split. text!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyswV9nuPJXF",
        "outputId": "51c7ff92-9e8a-44ce-aa8a-b6a4abfc1511"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['I', 'can', 'play', 'football']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "re.sub('[^A-z]',' ','I 123 can 45 play 67 football').split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz0wIRkRswOQ"
      },
      "source": [
        "### Удаление неинформативных слов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trdPOBM2jEMf"
      },
      "source": [
        "#### N-граммы\n",
        "\n",
        "<img src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--466CQV1q--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://thepracticaldev.s3.amazonaws.com/i/78nf1vryed8h1tz05fim.gif\" height=400>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYEBfCxLic3R",
        "outputId": "f626a422-c86b-45ff-ba22-31d1eb564d1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('all',), ('work',), ('and',), ('no',), ('play',)]\n",
            "[('all', 'work'), ('work', 'and'), ('and', 'no'), ('no', 'play'), ('play', 'makes')]\n"
          ]
        }
      ],
      "source": [
        "# Создаем список униграмм (последовательности из 1 слова) на основе токенов\n",
        "unigram = list(nltk.ngrams(tokens, 1))\n",
        "\n",
        "# Создаем список биграмм (последовательности из 2 слов) на основе токенов\n",
        "bigram = list(nltk.ngrams(tokens, 2))\n",
        "\n",
        "print(unigram[:5])\n",
        "print(bigram[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AFeZqejmWwN",
        "outputId": "dddf62c6-ec6a-4a34-ede6-1c375c162e69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Популярные униграммы:  [(('all',), 2), (('work',), 2), (('and',), 2), (('no',), 2), (('play',), 2)]\n",
            "Популярные биграммы:  [(('all', 'work'), 2), (('work', 'and'), 2), (('and', 'no'), 2), (('no', 'play'), 2), (('play', 'makes'), 1)]\n"
          ]
        }
      ],
      "source": [
        "# Импортируем класс FreqDist для подсчёта частоты элементов (униграмм и биграмм)\n",
        "from nltk import FreqDist\n",
        "\n",
        "print('Популярные униграммы: ', FreqDist(unigram).most_common(5))\n",
        "print('Популярные биграммы: ', FreqDist(bigram).most_common(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W3jJ56hnBFu"
      },
      "source": [
        "#### Стоп-слова"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIBwQ3nBnEfV",
        "outputId": "5d720077-678f-46d2-e86e-00478b399fcd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# Скачиваем набор стоп-слов (распространённых слов, которые обычно исключают из анализа)\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Импортируем список стоп-слов на разных языках из корпуса NLTK\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1nk-TqEslRl",
        "outputId": "89bb205a-7800-4233-a161-ef22994be86a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'haven', 'shan', 'she', 'between', 'before', \"needn't\", 'off', 'during', 'having', 'to', 'itself', 'be', \"hasn't\", 'hadn', 'where', 'of', 'with', 'because', 'mustn', 'yourself', 'me', 'was', 'mightn', 'had', 'no', 'which', 'very', 'will', 'i', 'ain', 'shouldn', 'at', 'until', 'yours', 'through', 'while', 'some', 'these', 'has', 's', \"hadn't\", 'themselves', \"wasn't\", 'again', 'in', 'been', 'all', 'yourselves', 'then', 're', 'most', \"couldn't\", 'now', 'm', 'by', \"wouldn't\", 'myself', \"doesn't\", 'but', 'is', 'theirs', 'from', \"you're\", 'him', 'you', \"you've\", \"she's\", 'am', 'than', 'himself', 'have', 'hasn', 'ourselves', 'don', \"don't\", 'both', 'do', 'should', 'ma', 'weren', 'there', 'this', 'were', 'those', 'after', 'did', 'own', 'doesn', 'does', 'it', 'into', 'the', \"it's\", 'we', 'few', 'their', 'its', 'hers', \"you'll\", 'up', 'can', \"isn't\", 'about', 'when', \"weren't\", \"won't\", 'once', 't', 'what', 'down', \"mustn't\", 'didn', 'any', 'my', 'more', 'only', 'being', 'd', 've', 'each', 'below', 'further', 'won', 'so', 'y', 'our', 'against', 'o', 'not', 'aren', 'whom', 'herself', 'other', 'just', 'are', 'as', 'that', \"you'd\", 'doing', 'he', \"shan't\", 'here', 'them', 'her', 'or', 'needn', 'if', 'who', 'wouldn', 'under', 'and', 'why', \"mightn't\", 'they', 'll', 'above', 'over', \"haven't\", \"aren't\", 'his', 'out', \"should've\", \"shouldn't\", \"that'll\", 'nor', \"didn't\", 'for', 'isn', 'wasn', 'on', 'same', 'couldn', 'your', 'how', 'ours', 'too', 'a', 'such', 'an'}\n"
          ]
        }
      ],
      "source": [
        "# Загружаем стоп-слова для английского языка и сохраняем их в множество\n",
        "stopWords = set(stopwords.words('english'))\n",
        "print(stopWords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFkfJm9ktAVa",
        "outputId": "4bce630d-b2ae-4e1b-c0a5-c8a555cfb7d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['work', 'play', 'makes', 'jack', 'dull', 'boy', ',', 'work', 'play']\n"
          ]
        }
      ],
      "source": [
        "print([word for word in tokens if word not in stopWords])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5QoqlHiS83f",
        "outputId": "ef530b0e-327e-4ddb-a84a-7b7d65f81299"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ],
      "source": [
        "# Импортируем модуль string для работы с символами\n",
        "import string\n",
        "\n",
        "# Посмотрим на строку, содержащую все знаки пунктуации\n",
        "print(string.punctuation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bh-MYv6e-skM"
      },
      "source": [
        "#### Стемминг vs Лемматизация\n",
        "* ‘Caring’ -> Лемматизация -> ‘Care’\n",
        "* ‘Caring’ -> Стемминг -> ‘Car’"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAUKc1oTiQjf"
      },
      "source": [
        "### Стемминг\n",
        "* процесс нахождения основы слова для заданного исходного слова"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRVu-TrON4sq"
      },
      "outputs": [],
      "source": [
        "# Импортируем стеммеры Porter и Snowball для приведения слов к базовой форме\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "\n",
        "# Список английских слов для стемминга\n",
        "words = [\"game\", \"gaming\", \"gamed\", \"games\", \"compacted\"]\n",
        "\n",
        "# Список русских слов для стемминга\n",
        "words_ru = ['корова', 'мальчики', 'мужчины', 'столом', 'убежала']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9HTGfsBN9eX",
        "outputId": "1d1bd62a-7dfd-4768-944d-8e9d95274998"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['game', 'game', 'game', 'game', 'compact']"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Создаем экземпляр стеммера Porter для английского языка\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# Применяем стемминг к каждому слову в списке words и преобразуем результат в список\n",
        "list(map(ps.stem, words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5qZkB-oODkW",
        "outputId": "58f143f6-2386-41db-e968-da1f5a8baff6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['коров', 'мальчик', 'мужчин', 'стол', 'убежа']"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Создаем экземпляр SnowballStemmer для русского языка\n",
        "ss = SnowballStemmer(language='russian')\n",
        "\n",
        "# Применяем стемминг к каждому слову в списке words_ru и преобразуем результат в список\n",
        "list(map(ss.stem, words_ru))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbTXbi9FJXr1"
      },
      "source": [
        "### Лематизация\n",
        "* процесс приведения словоформы к лемме — её нормальной (словарной) форме"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QF4nnEz00thb"
      },
      "outputs": [],
      "source": [
        "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
        "is no basis for a system of government.  Supreme executive power derives from\n",
        "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
        "\n",
        "raw_ru = \"\"\"Не существует научных доказательств в пользу эффективности НЛП, оно\n",
        "признано псевдонаукой. Систематические обзоры указывают, что НЛП основано на\n",
        "устаревших представлениях об устройстве мозга, несовместимо с современной\n",
        "неврологией и содержит ряд фактических ошибок.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mljfOAC4n21I",
        "outputId": "f494725c-6d3a-4fdb-805f-d997a9c13646"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Установка библиотеки pymorphy3 для морфологического анализа русского текста\n",
        "!pip install -q pymorphy3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Zez7jnXl5uJ",
        "outputId": "7a87c5af-5c9d-40aa-a1ed-738282dd7977"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "не существовать научный доказательство в польза эффективность нлп, оно\n",
            "признать псевдонаукой. систематический обзор указывают, что нлп основать на\n",
            "устаревший представление о устройство мозга, несовместимый с современной\n",
            "неврология и содержать ряд фактический ошибок.\n"
          ]
        }
      ],
      "source": [
        "import pymorphy3\n",
        "# Создаем экземпляр морфологического анализатора для русского языка\n",
        "morph = pymorphy3.MorphAnalyzer()\n",
        "\n",
        "# Разбиваем сырой текст raw_ru на слова и выполняем морфологический разбор каждого слова\n",
        "pymorphy_results = list(map(lambda x: morph.parse(x), raw_ru.split(' ')))\n",
        "\n",
        "# Для каждого разобранного слова берем его нормальную (начальную) форму и объединяем в строку\n",
        "print(' '.join([res[0].normal_form for res in pymorphy_results]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7MDqIvWib4O",
        "outputId": "d5611004-7972-40a1-c568-ee30098e4868"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DENNIS : listen , strange woman lie in pond distribute sword \n",
            " be no basis for a system of government .   Supreme executive power derive from \n",
            " a mandate from the masse , not from some farcical aquatic ceremony .\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Загружаем предобученную модель spaCy для обработки английского текста\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Обрабатываем сырой текст raw с помощью spaCy\n",
        "spacy_results = nlp(raw)\n",
        "\n",
        "# Для каждого токена извлекаем лемму (начальную форму слова) и объединяем в строку\n",
        "print(' '.join([token.lemma_ for token in spacy_results]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVC5gjCRk5bD"
      },
      "source": [
        "[Сравнение PyMorphy2 и PyMystem3](https://habr.com/ru/post/503420/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yim_NVYA6MeS"
      },
      "source": [
        "### Part-of-Speech"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2t_MamYKqbSk",
        "outputId": "699117b5-1b92-4d93-e95d-d7717b37c1ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('не', OpencorporaTag('PRCL')),\n",
              " ('существовать', OpencorporaTag('VERB,impf,intr sing,3per,pres,indc')),\n",
              " ('научный', OpencorporaTag('ADJF,Qual plur,gent')),\n",
              " ('доказательство', OpencorporaTag('NOUN,inan,neut plur,gent')),\n",
              " ('в', OpencorporaTag('PREP')),\n",
              " ('польза', OpencorporaTag('NOUN,inan,femn sing,accs')),\n",
              " ('эффективность', OpencorporaTag('NOUN,inan,femn sing,gent')),\n",
              " ('нлп,', OpencorporaTag('UNKN')),\n",
              " ('оно\\nпризнать', OpencorporaTag('PRTS,perf,past,pssv neut,sing'))]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Для первых 9 результатов морфологического разбора получаем пары (нормальная_форма, тег)\n",
        "[(res[0].normal_form, res[0].tag) for res in pymorphy_results[:9]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Doa85yw6JWea",
        "outputId": "81848aee-e4d5-46b3-fca6-fab91516f341"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('DENNIS', 'PROPN'),\n",
              " (':', 'PUNCT'),\n",
              " ('listen', 'VERB'),\n",
              " (',', 'PUNCT'),\n",
              " ('strange', 'ADJ'),\n",
              " ('woman', 'NOUN'),\n",
              " ('lie', 'VERB')]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Для первых 7 токенов извлекаем пары (лемма, часть речи) из результатов анализа spaCy\n",
        "[(token.lemma_, token.pos_) for token in spacy_results[:7]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_Slt2R76Mgk"
      },
      "source": [
        "### Named entities recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvB43ZHT6MhR",
        "outputId": "0aeb098f-3b9b-4c0e-fb3c-fc65f464241f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Apple 0 5 ORG\n",
            "U.K. 27 31 GPE\n",
            "$1 billion 44 54 MONEY\n"
          ]
        }
      ],
      "source": [
        "# Обрабатываем текст с помощью spaCy для извлечения именованных сущностей\n",
        "doc = nlp('Apple is looking at buying U.K. startup for $1 billion')\n",
        "\n",
        "# Выводим информацию о каждой найденной сущности: текст, позиции в строке и тип сущности\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0-_oFwwqAwN"
      },
      "source": [
        "## Часть 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIwDfmrYOMfi"
      },
      "source": [
        "### Задача классификации"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6DaLniC6MhY"
      },
      "source": [
        "#### 20 newsgroups\n",
        "Датасет с 18000 новостей, сгруппированных по 20 темам."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uS7IJNW6Mhb"
      },
      "outputs": [],
      "source": [
        "# Импортируем набор данных 20 Newsgroups из библиотеки scikit-learn\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Загружаем обучающую часть датасета 20 Newsgroups\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMbagpJE6Mhh",
        "outputId": "9d16162a-e3f2-4f63-84a9-9fd1f60644ed",
        "scrolled": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['alt.atheism',\n",
              " 'comp.graphics',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'comp.sys.ibm.pc.hardware',\n",
              " 'comp.sys.mac.hardware',\n",
              " 'comp.windows.x',\n",
              " 'misc.forsale',\n",
              " 'rec.autos',\n",
              " 'rec.motorcycles',\n",
              " 'rec.sport.baseball',\n",
              " 'rec.sport.hockey',\n",
              " 'sci.crypt',\n",
              " 'sci.electronics',\n",
              " 'sci.med',\n",
              " 'sci.space',\n",
              " 'soc.religion.christian',\n",
              " 'talk.politics.guns',\n",
              " 'talk.politics.mideast',\n",
              " 'talk.politics.misc',\n",
              " 'talk.religion.misc']"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Выводим список названий категорий (тем) в датасете 20 Newsgroups\n",
        "newsgroups_train.target_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QReW1K46Mhn",
        "outputId": "222ab4cc-a9b4-4298-8f10-3646977d1c84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(11314,)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "newsgroups_train.filenames.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZtRIQmNQ4H0"
      },
      "source": [
        "#### Рассмотрим подвыборку"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhwuCp5B6Mhz",
        "outputId": "41e863c7-59d7-43d4-8f42-8a7aebb6c058"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2034,)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Определяем список интересующих категорий из датасета 20 Newsgroups\n",
        "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
        "\n",
        "# Загружаем обучающую часть датасета только для указанных категорий\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
        "\n",
        "# Выводим форму массива с путями к файлам (количество примеров)\n",
        "newsgroups_train.filenames.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOREsv336MiA",
        "outputId": "2aacdaa4-9f0d-4ca3-a739-705e24f04ff3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "From: rych@festival.ed.ac.uk (R Hawkes)\n",
            "Subject: 3DS: Where did all the texture rules go?\n",
            "Lines: 21\n",
            "\n",
            "Hi,\n",
            "\n",
            "I've noticed that if you only save a model (with all your mapping planes\n",
            "positioned carefully) to a .3DS file that when you reload it after restarting\n",
            "3DS, they are given a default position and orientation.  But if you save\n",
            "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
            "know why this information is not stored in the .3DS file?  Nothing is\n",
            "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
            "I'd like to be able to read the texture rule information, does anyone have \n",
            "the format for the .PRJ file?\n",
            "\n",
            "Is the .CEL file format available from somewhere?\n",
            "\n",
            "Rych\n",
            "\n",
            "======================================================================\n",
            "Rycharde Hawkes\t\t\t\temail: rych@festival.ed.ac.uk\n",
            "Virtual Environment Laboratory\n",
            "Dept. of Psychology\t\t\tTel  : +44 31 650 3426\n",
            "Univ. of Edinburgh\t\t\tFax  : +44 31 667 0150\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(newsgroups_train.data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBnDG-TN6MiF",
        "outputId": "5c713290-6871-4c94-c503-117814d2d43e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 3, 2, 0, 2, 0, 2, 1, 2, 1])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Выводим первые 10 меток классов (целевых значений) из обучающего набора\n",
        "newsgroups_train.target[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XlZYpodRYDI"
      },
      "source": [
        "#### TF-IDF(напоминание)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohUk2n3jRbNp"
      },
      "source": [
        "$n_{\\mathbb{d}\\mathbb{w}}$ - число вхождений слова $\\mathbb{w}$ в документ $\\mathbb{d}$;<br>\n",
        "$N_{\\mathbb{w}}$ - число документов, содержащих $\\mathbb{w}$;<br>\n",
        "$N$ - число документов; <br><br>\n",
        "\n",
        "$p(\\mathbb{w}, \\mathbb{d}) = N_{\\mathbb{w}} / N$ - вероятность наличия слова $\\mathbb{w}$ в любом документе $\\mathbb{d}$\n",
        "<br>\n",
        "$P(\\mathbb{w}, \\mathbb{d}, n_{\\mathbb{d}\\mathbb{w}}) = (N_{\\mathbb{w}} / N)^{n_{\\mathbb{d}\\mathbb{w}}}$ - вероятность встретить $n_{\\mathbb{d}\\mathbb{w}}$ раз слово $\\mathbb{w}$ в документе $\\mathbb{d}$<br><br>\n",
        "\n",
        "$-\\log{P(\\mathbb{w}, \\mathbb{d}, n_{\\mathbb{d}\\mathbb{w}})} = n_{\\mathbb{d}\\mathbb{w}} \\cdot \\log{(N / N_{\\mathbb{w}})} = TF(\\mathbb{w}, \\mathbb{d}) \\cdot IDF(\\mathbb{w})$<br><br>\n",
        "\n",
        "$TF(\\mathbb{w}, \\mathbb{d}) = n_{\\mathbb{d}\\mathbb{w}}$ - term frequency;<br>\n",
        "$IDF(\\mathbb{w}) = \\log{(N /N_{\\mathbb{w}})}$ - inverted document frequency;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQvcMiFH6MiM"
      },
      "source": [
        "#### Давайте векторизуем эти тексты с помощью TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98LLAoZO6MiU"
      },
      "outputs": [],
      "source": [
        "# Импортируем TfidfVectorizer для преобразования текстовых данных в матрицу TF-IDF признаков\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baXLU0lj6MiY"
      },
      "source": [
        "#### Некоторые параметры TfidfVectorizer:\n",
        "* input : string {‘filename’, ‘file’, ‘content’} — определяет тип входных данных (по умолчанию 'content')\n",
        "* lowercase : boolean, default True — преобразует текст к нижнему регистру, если True\n",
        "* preprocessor : callable or None (default) — пользовательская функция предобработки текста\n",
        "* tokenizer : callable or None (default) — пользовательская функция токенизации текста\n",
        "* stop_words : string {‘english’}, list, or None (default) — задает стоп-слова для удаления\n",
        "* ngram_range : tuple (min_n, max_n) — диапазон длин последовательностей слов (n-грамм)\n",
        "* max_df : float in range [0.0, 1.0] or int, default=1.0 — исключает слова, которые слишком частые\n",
        "* min_df : float in range [0.0, 1.0] or int, default=1 — исключает слова, которые слишком редкие\n",
        "* max_features : int or None, default=None — ограничивает количество признаков (слов) в матрице"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-m81BJxZFwJ"
      },
      "source": [
        "#### Перебор параметров"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_f7padHL6MiZ",
        "outputId": "7f6696e3-3816-4887-f55e-eb6b93937949"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2034, 34118)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Создаем экземпляр TfidfVectorizer (по умолчанию преобразует текст к нижнему регистру)\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Преобразуем текстовые данные в матрицу TF-IDF признаков\n",
        "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
        "\n",
        "# Выводим размер матрицы: количество документов x количество уникальных слов\n",
        "vectors.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nf2s4HCY6Mie",
        "outputId": "8554ec5f-3b73-461f-ea4c-06b218bf5ffe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2034, 42307)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Создаем TfidfVectorizer с отключенным преобразованием в нижний регистр\n",
        "vectorizer = TfidfVectorizer(lowercase=False)\n",
        "\n",
        "# Преобразуем текстовые данные в матрицу TF-IDF признаков\n",
        "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
        "\n",
        "# Выводим размер матрицы: количество документов x количество уникальных слов\n",
        "vectors.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hwlTWapZR8M",
        "outputId": "372144c2-93d5-4560-da47-58a2e7d30992"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['00', '000', '0000', '00000', '000000', '000005102000', '000021',\n",
              "       '000062David42', '0000VEC', '0001'], dtype=object)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Выводим первые 10 названий признаков (слов) из векторизатора\n",
        "vectorizer.get_feature_names_out()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYfpk0ds6Mij",
        "outputId": "f6c98904-b598-40b3-bd41-25d964a3d4ee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2034, 9)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Создаем TfidfVectorizer с фильтрацией слов: учитываются только те, что встречаются как минимум в 80% документов\n",
        "vectorizer = TfidfVectorizer(min_df=0.8)\n",
        "\n",
        "# Преобразуем текстовые данные в матрицу TF-IDF признаков\n",
        "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
        "\n",
        "# Выводим размер полученной матрицы: количество документов x количество отфильтрованных уникальных слов\n",
        "vectors.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ookt99atZ8sS",
        "outputId": "70fa95d3-f9cc-4f36-ffa6-fd6b33e05412"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['and', 'from', 'in', 'lines', 'of', 'organization', 'subject',\n",
              "       'the', 'to'], dtype=object)"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Получаем массив названий признаков (уникальных слов) после трансформации с текущим векторизатором\n",
        "vectorizer.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e_h6c0X6Mim",
        "outputId": "b6bdf82a-5d27-403f-a3e0-f8ffd289afd0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2034, 2391)"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Создаем TfidfVectorizer с фильтрацией слов:\n",
        "# учитываем только слова, встречающиеся как минимум в 1% документов и не более чем в 80%\n",
        "vectorizer = TfidfVectorizer(min_df=0.01, max_df=0.8)\n",
        "\n",
        "# Преобразуем текстовые данные в матрицу TF-IDF признаков\n",
        "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
        "\n",
        "# Выводим размер полученной матрицы: количество документов x количество отфильтрованных уникальных слов\n",
        "vectors.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUhOyx4NihL0",
        "outputId": "e5fd46c4-29c7-40be-fdc7-e6ee86643564"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2034, 1236)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Создаем TfidfVectorizer с настройкой ngram_range: учитываем униграммы, биграммы и триграммы\n",
        "# Также фильтруем слова, встречающиеся как минимум в 3% и не более чем в 90% документов\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 3), min_df=0.03, max_df=0.9)\n",
        "\n",
        "# Преобразуем текстовые данные в матрицу TF-IDF признаков\n",
        "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
        "\n",
        "# Выводим размер полученной матрицы: количество документов x количество отфильтрованных n-грамм\n",
        "vectors.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "7074cdSF6MjC",
        "outputId": "2c49bcbc-fcbf-43fd-c49d-753b6c401128"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'oh , think landed miracle work , thirst hunger come conference bird'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Подключаем обработку стоп-слов и лемматизацию из NLTK\n",
        "from nltk.corpus import stopwords\n",
        "stopWords = set(stopwords.words('english'))  # Загружаем английские стоп-слова\n",
        "nltk.download('wordnet')  # Скачиваем ресурс wordnet для лемматизации\n",
        "wnl = nltk.WordNetLemmatizer()  # Создаем экземпляр лемматизатора\n",
        "\n",
        "# Функция предобработки текста: приведение к нижнему регистру, удаление стоп-слов, лемматизация\n",
        "def preproc_nltk(text):\n",
        "    # Опционально: удаление пунктуации через регулярные выражения (закомментировано)\n",
        "    # text = re.sub(f'[{string.punctuation}]', ' ', text)\n",
        "    return ' '.join([\n",
        "        wnl.lemmatize(word)  # Лемматизируем слово\n",
        "        for word in word_tokenize(text.lower())  # Разбиваем на слова и приводим к нижнему регистру\n",
        "        if word not in stopWords  # Исключаем стоп-слова\n",
        "    ])\n",
        "\n",
        "# Пример строки для обработки\n",
        "st = \"Oh, I think I ve landed Where there are miracles at work,  For the thirst and for the hunger Come the conference of birds\"\n",
        "\n",
        "# Применяем функцию предобработки к примеру\n",
        "preproc_nltk(st)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIW3hCSy6MjX",
        "outputId": "278927e1-315d-4043-d01c-b5d48659858f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 8.51 s, sys: 43.7 ms, total: 8.55 s\n",
            "Wall time: 8.64 s\n"
          ]
        }
      ],
      "source": [
        "# %%time  # Закомментировано: магическая команда Jupyter для измерения времени выполнения ячейки\n",
        "vectorizer = TfidfVectorizer(preprocessor=preproc_nltk)  # Используем пользовательскую функцию предобработки текста\n",
        "\n",
        "# Преобразуем текстовые данные в матрицу TF-IDF признаков с учетом предобработки\n",
        "vectors = vectorizer.fit_transform(newsgroups_train.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "lUmyAzJEB1SU",
        "outputId": "73b8b4da-7768-40e5-f448-3b87642c2d8e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'oh , I think I land miracle work ,   thirst hunger come conference bird'"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Загружаем модель spaCy для обработки английского языка\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Копируем текстовые данные из датасета для последующей обработки\n",
        "texts = newsgroups_train.data.copy()\n",
        "\n",
        "# Функция предобработки текста с использованием spaCy:\n",
        "# лемматизация и удаление стоп-слов\n",
        "def preproc_spacy(text):\n",
        "    spacy_results = nlp(text)  # Обрабатываем текст с помощью spaCy\n",
        "    return ' '.join([\n",
        "        token.lemma_  # Берем лемму слова\n",
        "        for token in spacy_results\n",
        "        if token.lemma_ not in stopWords  # Исключаем стоп-слова\n",
        "    ])\n",
        "\n",
        "# Применяем функцию предобработки к примеру текста\n",
        "preproc_spacy(st)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0C5Ent0nG5zj",
        "outputId": "a7191a8c-5ec3-4bbd-ffa2-180bbc13c47b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 16.3 s, sys: 523 ms, total: 16.8 s\n",
            "Wall time: 1min 28s\n"
          ]
        }
      ],
      "source": [
        "# %%time  # Закомментировано: магическая команда Jupyter для измерения времени выполнения ячейки\n",
        "\n",
        "# Инициализируем список для хранения обработанных текстов\n",
        "new_texts = []\n",
        "\n",
        "# Обрабатываем тексты пакетно с помощью spaCy (ускоренная обработка):\n",
        "# - batch_size=32: обрабатываем по 32 документа за раз\n",
        "# - n_process=3: используем 3 процесса параллельно\n",
        "# - отключаем ненужные компоненты (\"parser\", \"ner\") для ускорения\n",
        "for doc in nlp.pipe(texts, batch_size=32, n_process=3, disable=[\"parser\", \"ner\"]):\n",
        "    # Лемматизируем и исключаем стоп-слова, затем объединяем в строку\n",
        "    new_texts.append(' '.join([tok.lemma_ for tok in doc if tok.lemma_ not in stopWords]))\n",
        "\n",
        "# Создаем экземпляр TfidfVectorizer без предобработки\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Преобразуем предобработанные тексты в матрицу TF-IDF признаков\n",
        "vectors = vectorizer.fit_transform(new_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lXAPHZA6Mj0"
      },
      "source": [
        "#### Итоговая модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZYcRkQ86Mj1",
        "outputId": "d51165ee-c287-4912-8944-bc5abcb7843b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['000', 'attempt', 'choose', 'engineering', 'human', 'look',\n",
              "       'of this', 'report', 'technology', 'understand'], dtype=object)"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Создаем TfidfVectorizer с настройками:\n",
        "# - учитываем униграммы, биграммы и триграммы (ngram_range=(1, 3))\n",
        "# - исключаем слова, встречающиеся более чем в 50% документов (max_df=0.5)\n",
        "# - ограничиваем количество признаков до 1000 (max_features=1000)\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_df=0.5, max_features=1000)\n",
        "\n",
        "# Преобразуем предобработанные тексты в матрицу TF-IDF признаков\n",
        "vectors = vectorizer.fit_transform(new_texts)\n",
        "\n",
        "# Получаем список названий признаков (n-грамм) и выводим каждый 100-й элемент для демонстрации\n",
        "vectorizer.get_feature_names_out()[::100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQHTlj3q6Mj6"
      },
      "source": [
        "#### Можем посмотреть на косинусную меру между векторами"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "VsVQhR9y6Mj8",
        "outputId": "3594b9f6-2553-47c1-9628-7cb57ae35b81"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>scipy.sparse._csr.csr_matrix</b><br/>def __init__(arg1, shape=None, dtype=None, copy=False)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/scipy/sparse/_csr.py</a>Compressed Sparse Row matrix.\n",
              "\n",
              "This can be instantiated in several ways:\n",
              "    csr_matrix(D)\n",
              "        where D is a 2-D ndarray\n",
              "\n",
              "    csr_matrix(S)\n",
              "        with another sparse array or matrix S (equivalent to S.tocsr())\n",
              "\n",
              "    csr_matrix((M, N), [dtype])\n",
              "        to construct an empty matrix with shape (M, N)\n",
              "        dtype is optional, defaulting to dtype=&#x27;d&#x27;.\n",
              "\n",
              "    csr_matrix((data, (row_ind, col_ind)), [shape=(M, N)])\n",
              "        where ``data``, ``row_ind`` and ``col_ind`` satisfy the\n",
              "        relationship ``a[row_ind[k], col_ind[k]] = data[k]``.\n",
              "\n",
              "    csr_matrix((data, indices, indptr), [shape=(M, N)])\n",
              "        is the standard CSR representation where the column indices for\n",
              "        row i are stored in ``indices[indptr[i]:indptr[i+1]]`` and their\n",
              "        corresponding values are stored in ``data[indptr[i]:indptr[i+1]]``.\n",
              "        If the shape parameter is not supplied, the matrix dimensions\n",
              "        are inferred from the index arrays.\n",
              "\n",
              "Attributes\n",
              "----------\n",
              "dtype : dtype\n",
              "    Data type of the matrix\n",
              "shape : 2-tuple\n",
              "    Shape of the matrix\n",
              "ndim : int\n",
              "    Number of dimensions (this is always 2)\n",
              "nnz\n",
              "size\n",
              "data\n",
              "    CSR format data array of the matrix\n",
              "indices\n",
              "    CSR format index array of the matrix\n",
              "indptr\n",
              "    CSR format index pointer array of the matrix\n",
              "has_sorted_indices\n",
              "has_canonical_format\n",
              "T\n",
              "\n",
              "Notes\n",
              "-----\n",
              "\n",
              "Sparse matrices can be used in arithmetic operations: they support\n",
              "addition, subtraction, multiplication, division, and matrix power.\n",
              "\n",
              "Advantages of the CSR format\n",
              "  - efficient arithmetic operations CSR + CSR, CSR * CSR, etc.\n",
              "  - efficient row slicing\n",
              "  - fast matrix vector products\n",
              "\n",
              "Disadvantages of the CSR format\n",
              "  - slow column slicing operations (consider CSC)\n",
              "  - changes to the sparsity structure are expensive (consider LIL or DOK)\n",
              "\n",
              "Canonical Format\n",
              "    - Within each row, indices are sorted by column.\n",
              "    - There are no duplicate entries.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "\n",
              "&gt;&gt;&gt; import numpy as np\n",
              "&gt;&gt;&gt; from scipy.sparse import csr_matrix\n",
              "&gt;&gt;&gt; csr_matrix((3, 4), dtype=np.int8).toarray()\n",
              "array([[0, 0, 0, 0],\n",
              "       [0, 0, 0, 0],\n",
              "       [0, 0, 0, 0]], dtype=int8)\n",
              "\n",
              "&gt;&gt;&gt; row = np.array([0, 0, 1, 2, 2, 2])\n",
              "&gt;&gt;&gt; col = np.array([0, 2, 2, 0, 1, 2])\n",
              "&gt;&gt;&gt; data = np.array([1, 2, 3, 4, 5, 6])\n",
              "&gt;&gt;&gt; csr_matrix((data, (row, col)), shape=(3, 3)).toarray()\n",
              "array([[1, 0, 2],\n",
              "       [0, 0, 3],\n",
              "       [4, 5, 6]])\n",
              "\n",
              "&gt;&gt;&gt; indptr = np.array([0, 2, 3, 6])\n",
              "&gt;&gt;&gt; indices = np.array([0, 2, 2, 0, 1, 2])\n",
              "&gt;&gt;&gt; data = np.array([1, 2, 3, 4, 5, 6])\n",
              "&gt;&gt;&gt; csr_matrix((data, indices, indptr), shape=(3, 3)).toarray()\n",
              "array([[1, 0, 2],\n",
              "       [0, 0, 3],\n",
              "       [4, 5, 6]])\n",
              "\n",
              "Duplicate entries are summed together:\n",
              "\n",
              "&gt;&gt;&gt; row = np.array([0, 1, 2, 0])\n",
              "&gt;&gt;&gt; col = np.array([0, 1, 1, 0])\n",
              "&gt;&gt;&gt; data = np.array([1, 2, 4, 8])\n",
              "&gt;&gt;&gt; csr_matrix((data, (row, col)), shape=(3, 3)).toarray()\n",
              "array([[9, 0, 0],\n",
              "       [0, 2, 0],\n",
              "       [0, 4, 0]])\n",
              "\n",
              "As an example of how to construct a CSR matrix incrementally,\n",
              "the following snippet builds a term-document matrix from texts:\n",
              "\n",
              "&gt;&gt;&gt; docs = [[&quot;hello&quot;, &quot;world&quot;, &quot;hello&quot;], [&quot;goodbye&quot;, &quot;cruel&quot;, &quot;world&quot;]]\n",
              "&gt;&gt;&gt; indptr = [0]\n",
              "&gt;&gt;&gt; indices = []\n",
              "&gt;&gt;&gt; data = []\n",
              "&gt;&gt;&gt; vocabulary = {}\n",
              "&gt;&gt;&gt; for d in docs:\n",
              "...     for term in d:\n",
              "...         index = vocabulary.setdefault(term, len(vocabulary))\n",
              "...         indices.append(index)\n",
              "...         data.append(1)\n",
              "...     indptr.append(len(indices))\n",
              "...\n",
              "&gt;&gt;&gt; csr_matrix((data, indices, indptr), dtype=int).toarray()\n",
              "array([[2, 1, 0, 0],\n",
              "       [0, 1, 1, 1]])</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 370);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ],
            "text/plain": [
              "scipy.sparse._csr.csr_matrix"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Импортируем библиотеку numpy для работы с массивами и матрицами\n",
        "import numpy as np\n",
        "# Импортируем функцию norm для вычисления нормы вектора из линейной алгебры numpy\n",
        "from numpy.linalg import norm\n",
        "\n",
        "# Выводим тип объекта vectors (обычно это разреженная матрица типа scipy.sparse)\n",
        "type(vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VA8Fn5I46Mi0",
        "outputId": "ae3fb0fa-ce41-4354-ec8e-ae0d90548fb0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "52"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Преобразуем первый документ из разреженной матрицы в плотный вектор (numpy matrix)\n",
        "vector = vectors.todense()[0]\n",
        "\n",
        "# Считаем количество ненулевых элементов в векторе (т.е. сколько уникальных n-грамм присутствует в документе)\n",
        "(vector != 0).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jo4fKtAXqCl-",
        "outputId": "7c9fe16a-e534-45c5-fae7-ac617383f86e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 1000)"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kweWqI8ztDwC",
        "outputId": "6f8066d1-b7f9-41e8-e9aa-b5df8e760480"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "89.8023598820059"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Вычисляем среднее количество ненулевых элементов по всем векторам:\n",
        "# - преобразуем каждый вектор в плотный формат\n",
        "# - считаем количество ненулевых признаков для каждого документа\n",
        "# - находим среднее значение по всем документам\n",
        "np.mean(list(map(lambda x: (x != 0).sum(), vectors.todense())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tS2G0ZZC6MkN",
        "outputId": "e1e1dc8f-f48d-4394-af60-44da21d774c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2034, 1000)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Преобразуем разреженную матрицу TF-IDF в плотный массив (матрицу)\n",
        "dense_vectors = vectors.todense()\n",
        "\n",
        "# Выводим размерность полученной плотной матрицы: количество документов x количество признаков\n",
        "dense_vectors.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGgb5aP76MkU"
      },
      "outputs": [],
      "source": [
        "# Функция для вычисления косинусного сходства между двумя векторами v1 и v2\n",
        "def cosine_sim(v1, v2):\n",
        "    # v1, v2 — одномерные векторы размерности (1 x dim)\n",
        "    return np.array(v1 @ v2.T / norm(v1) / norm(v2))[0][0]  # Вычисляем косинус угла между векторами"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_FrKM6k6MkY",
        "outputId": "6efe418a-ad83-40b5-dd1e-201c518fbf3c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Вычисляем косинусное сходство первого документа с самим собой\n",
        "cosine_sim(dense_vectors[0], dense_vectors[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1XF-isH6Mkh"
      },
      "outputs": [],
      "source": [
        "# Вычисляем косинусное сходство между первым документом и первыми 10 документами\n",
        "cosines = []\n",
        "for i in range(10):\n",
        "    # Для каждого из первых 10 документов считаем сходство с нулевым (первым) документом\n",
        "    cosines.append(cosine_sim(dense_vectors[0], dense_vectors[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODwgYEbe6Mkl",
        "outputId": "05a42841-673f-43e8-ce18-b901f4d22af1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1.0,\n",
              " 0.04191279776414235,\n",
              " 0.00586838361101993,\n",
              " 0.09771238093526101,\n",
              " 0.0706091645327028,\n",
              " 0.06745764842966308,\n",
              " 0.026714182362747592,\n",
              " 0.22853760897260952,\n",
              " 0.03163642012466395,\n",
              " 0.0692866259316149]"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Выводим список косинусных сходств между первым документом и первыми 10 документами\n",
        "cosines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRZyJP3c6Mkq"
      },
      "source": [
        "#### Обучим любую известную модель на полученных признаках"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RDfl72A6Mks",
        "outputId": "f13382c8-8448-48c6-e529-858b4f7a6a3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((1627,), (407,))"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import svm # svm для использования метода опорных векторов\n",
        "from sklearn.linear_model import SGDClassifier # SGDClassifier для стохастического градиентного спуска\n",
        "\n",
        "# Разделяем данные на обучающую и тестовую выборки (80% обучение, 20% тестирование)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    dense_vectors,\n",
        "    newsgroups_train.target,\n",
        "    test_size=0.2,\n",
        "    random_state=0  # Фиксируем случайное разбиение\n",
        ")\n",
        "y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oV55UkMEFyZT"
      },
      "outputs": [],
      "source": [
        "X_train = np.asarray(X_train)\n",
        "y_train = np.asarray(y_train)\n",
        "X_test= np.asarray(X_test)\n",
        "y_test= np.asarray(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "id": "vjfwduNp6Mlo",
        "outputId": "09ce037d-0109-4565-c337-c0ac32c23035"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 1.37 s, sys: 14.8 ms, total: 1.38 s\n",
            "Wall time: 1.41 s\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>SVC</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>SVC()</pre></div> </div></div></div></div>"
            ],
            "text/plain": [
              "SVC()"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "svc = svm.SVC()\n",
        "svc.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6Evwipx6Mlv",
        "outputId": "e9168aff-720d-403b-88cb-590068e3b4e9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9262899262899262"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_score(y_test, svc.predict(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EBZRbXT6Mly",
        "outputId": "d5431177-0212-4175-86a4-28fcec2647a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.914004914004914"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sgd = SGDClassifier()\n",
        "sgd.fit(X_train, y_train)\n",
        "accuracy_score(y_test, sgd.predict(X_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5OAwBJ0LuPb"
      },
      "source": [
        "### Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKLGAYPvPJcJ",
        "outputId": "2700d52c-98d3-4765-b233-ed29ecc31fd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "# Импортируем модуль для загрузки предобученных эмбеддингов из библиотеки gensim\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Загружаем предобученные векторные представления слов (эмбеддинги) с названием 'glove-twitter-25'\n",
        "embeddings_pretrained = api.load('glove-twitter-25')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmH3vc7FSCuP"
      },
      "outputs": [],
      "source": [
        "# Импортируем модель Word2Vec для обучения собственных эмбеддингов слов\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Предобрабатываем тексты с помощью ранее определенной функции preproc_nltk и разбиваем на слова\n",
        "proc_words = [preproc_nltk(text).split() for text in newsgroups_train.data]\n",
        "\n",
        "# Обучаем модель Word2Vec на обработанных текстах:\n",
        "# - vector_size=100: размерность вектора слова\n",
        "# - min_count=3: игнорируем редкие слова, встречающиеся реже 3 раз\n",
        "# - window=3: размер окна контекста вокруг слова\n",
        "embeddings_trained = Word2Vec(proc_words, vector_size=100, min_count=3, window=3).wv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Lq20widPJcO"
      },
      "outputs": [],
      "source": [
        "def vectorize_sum(comment, embeddings):\n",
        "    \"\"\"\n",
        "    Преобразует предобработанный комментарий в вектор,\n",
        "    представляющий собой сумму эмбеддингов входящих в него слов.\n",
        "\n",
        "    Параметры:\n",
        "    - comment: текст комментария (строка)\n",
        "    - embeddings: модель эмбеддингов (например, Word2Vec или GloVe)\n",
        "\n",
        "    Возвращает:\n",
        "    - вектор текста (numpy массив)\n",
        "    \"\"\"\n",
        "    # Получаем размерность эмбеддингов\n",
        "    embedding_dim = embeddings.vectors.shape[1]\n",
        "\n",
        "    # Инициализируем нулевой вектор признаков\n",
        "    features = np.zeros([embedding_dim], dtype='float32')\n",
        "\n",
        "    # Предобрабатываем комментарий и разбиваем на слова\n",
        "    for word in preproc_nltk(comment).split():\n",
        "        if word in embeddings:\n",
        "            # Добавляем вектор слова к общему представлению текста\n",
        "            features += embeddings[word]\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1KaMKBHsSHd",
        "outputId": "417f8500-485d-4299-d8f2-c47f00e20efd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "13553"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Выводим количество уникальных слов в словаре обученной модели эмбеддингов Word2Vec\n",
        "len(embeddings_trained.index_to_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krjpVRsLPJcf",
        "outputId": "37d55ff6-ea6e-4c4b-efb5-c77c8e31d1ea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((1627, 25), (407, 25))"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Преобразуем все тексты из датасета в векторное представление с помощью предобученных эмбеддингов\n",
        "X_wv = np.stack([vectorize_sum(text, embeddings_pretrained) for text in newsgroups_train.data])\n",
        "\n",
        "X_train_wv, X_test_wv, y_train, y_test = train_test_split(\n",
        "    X_wv,\n",
        "    newsgroups_train.target,\n",
        "    test_size=0.2,\n",
        "    random_state=0\n",
        ")\n",
        "X_train_wv.shape, X_test_wv.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWhQ007PPJcn",
        "outputId": "be0d0925-57e4-4a4c-d1e2-aa302889fd81"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.6953316953316954"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "clf = LogisticRegression(max_iter=5000)\n",
        "\n",
        "# Обучаем модель на обучающей выборке с векторным представлением текстов\n",
        "wv_model = clf.fit(X_train_wv, y_train)\n",
        "accuracy_score(y_test, wv_model.predict(X_test_wv))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oT_Rr4nOUUu8",
        "outputId": "6181e25d-7c0e-44eb-e3b8-5fcdd5c40010"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((1627, 100), (407, 100))"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Преобразуем все тексты из датасета в векторное представление с помощью обученных эмбеддингов (Word2Vec)\n",
        "X_wv = np.stack([vectorize_sum(text, embeddings_trained) for text in newsgroups_train.data])\n",
        "\n",
        "\n",
        "X_train_wv, X_test_wv, y_train, y_test = train_test_split(\n",
        "    X_wv,\n",
        "    newsgroups_train.target,\n",
        "    test_size=0.2,\n",
        "    random_state=0  # Фиксируем случайное разбиение\n",
        ")\n",
        "X_train_wv.shape, X_test_wv.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVMeZBLbVMjO",
        "outputId": "0b2263a3-b71e-4694-b362-f47dd75ecf46"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.8353808353808354"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "clf = LogisticRegression(max_iter=10000)\n",
        "\n",
        "# Обучаем модель на обучающей выборке с векторным представлением текстов на основе обученных эмбеддингов\n",
        "wv_model = clf.fit(X_train_wv, y_train)\n",
        "accuracy_score(y_test, wv_model.predict(X_test_wv))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {
        "height": "11.8333px",
        "width": "160px"
      },
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "339.717px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}