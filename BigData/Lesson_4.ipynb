{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4c2dcd7",
   "metadata": {},
   "source": [
    "# Лекция 4 \n",
    "# Практика Spark - ETL-пайплайн для отзывов Кинопоиска"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78922001",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "## Введение в ETL-процессы\n",
    "\n",
    "### Что такое ETL?\n",
    "ETL — процесс подготовки данных для анализа и машинного обучения:\n",
    "\n",
    "```\n",
    "Raw Data → Extract → Transform → Load → Analytics/ML\n",
    "(Сырые данные → Извлечение → Трансформация → Загрузка → Аналитика/ML)\n",
    "```\n",
    "\n",
    "В нашем случае:\n",
    "```\n",
    "131583 текстовых файлов → Spark DataFrame → Очистка → Parquet → Готово для анализа\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccd9a4b",
   "metadata": {},
   "source": [
    "## 1. Введение в структурированные данные в Spark\n",
    "\n",
    "### 1.1. Эволюция Spark API: от RDD к DataFrame\n",
    "```\n",
    "Эволюция абстракций данных в Spark:\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    УРОВЕНЬ АБСТРАКЦИИ                       │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│  DataFrame/Dataset (Высокий уровень)                        │\n",
    "│  • Декларативный SQL-подобный синтаксис                     │\n",
    "│  • Автоматическая оптимизация через Catalyst                │\n",
    "│  • Типизированные, структурированные данные                 │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│  DataFrame API (Средний уровень)                            │\n",
    "│  • Табличное представление                                  │\n",
    "│  • Schema-on-read                                           │\n",
    "│  • Оптимизации Tungsten                                     │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│  RDD API (Низкий уровень)                                   │\n",
    "│  • Императивный стиль                                       │\n",
    "│  • Работа с неструктурированными данными                    │\n",
    "│  • Полный контроль над execution plan                       │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac893a9e",
   "metadata": {},
   "source": [
    "### 1.2. Архитектурное сравнение RDD vs DataFrame\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Запрос пользователя] --> B{RDD API}\n",
    "    A --> C{DataFrame API}\n",
    "    \n",
    "    B --> D[Ручная оптимизация]\n",
    "    B --> E[Functional Programming]\n",
    "    B --> F[Python/JVM overhead]\n",
    "    \n",
    "    C --> G[Catalyst Optimizer]\n",
    "    C --> H[Tungsten Engine]\n",
    "    C --> I[Whole-Stage Codegen]\n",
    "    \n",
    "    G --> J[Логическая оптимизация]\n",
    "    G --> K[Физическая оптимизация]\n",
    "    G --> L[Cost-based оптимизация]\n",
    "    \n",
    "    H --> M[Off-heap память]\n",
    "    H --> N[Векторизованные операции]\n",
    "    \n",
    "    I --> O[Генерация Java bytecode]\n",
    "    \n",
    "    style C fill:#e1f5e1,stroke:#333,color:#000\n",
    "    style G fill:#e1f5e1,stroke:#333,color:#000\n",
    "    style H fill:#e1f5e1,stroke:#333,color:#000\n",
    "    style I fill:#e1f5e1,stroke:#333,color:#000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b360aa05",
   "metadata": {},
   "source": [
    "## 2. Архитектурные различия: RDD vs DataFrame\n",
    "\n",
    "### 2.1. Структурное сравнение\n",
    "\n",
    "| Аспект | RDD (Resilient Distributed Dataset) | *DataFrame |\n",
    "|--------|------------------------------------------|---------------|\n",
    "| Уровень абстракции | Низкоуровневый | Высокоуровневый |\n",
    "| Структура данных | Произвольные объекты Python/JVM | Таблица со схемой (строки и колонки) |\n",
    "| Оптимизация | Нет автоматической оптимизации | Catalyst Optimizer + Tungsten |\n",
    "| Синтаксис | Функциональный (map, filter, reduce) | Декларативный (SQL-подобный) |\n",
    "| Типизация | Динамическая (Python) / Статическая (Scala) | Статическая (Schema-on-read) |\n",
    "| Производительность | Медленнее (Python overhead) | Быстрее (JVM-оптимизации) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e576e9",
   "metadata": {},
   "source": [
    "### 2.2. Визуализация архитектурных различий\n",
    "\n",
    "```\n",
    "RDD ПРИМЕР:\n",
    "val rdd = sc.textFile(\"data.txt\")           // RDD[String]\n",
    "  .flatMap(_.split(\" \"))                    // RDD[String]  \n",
    "  .map(word => (word, 1))                   // RDD[(String, Int)]\n",
    "  .reduceByKey(_ + _)                       // Shuffle\n",
    "  .collect()\n",
    "\n",
    "DataFrame ПРИМЕР:\n",
    "val df = spark.read.text(\"data.txt\")        // DataFrame[value: String]\n",
    "  .select(explode(split(col(\"value\"), \" \"))) // DataFrame[word: String]\n",
    "  .groupBy(\"word\").count()                  // DataFrame[word: String, count: Long]\n",
    "  .collect()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a8b941",
   "metadata": {},
   "source": [
    "### 2.3. Tungsten Engine: Секретное оружие Spark \n",
    "\n",
    "Что такое Tungsten и почему он важен\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Apache Spark: исполнение запросов] --> B{До Tungsten}\n",
    "    A --> C{С Tungsten}\n",
    "\n",
    "    B --> B1[Много Java-объектов в куче JVM]\n",
    "    B --> B2[Высокий overhead на объект и ссылку]\n",
    "    B --> B3[Частые GC-паузы]\n",
    "    B --> B4[Плохая кэш-локальность]\n",
    "\n",
    "    C --> C1[\"Бинарный формат данных (как C-структуры)\"]\n",
    "    C --> C2[\"Off-heap управление памятью\"]\n",
    "    C --> C3[\"Векторизованные операции (колоночная обработка, SIMD)\"]\n",
    "    C --> C4[\"Whole-stage codegen (генерация Java bytecode)\"]\n",
    "\n",
    "    C1 --> D1[2-5x меньше памяти]\n",
    "    C2 --> D2[Меньше нагрузки на GC и пауз]\n",
    "    C3 --> D3[Рост скорости сканов и агрегаций]\n",
    "    C4 --> D4[Меньше аллокаций и вызовов, до 10-100x быстрее]\n",
    "\n",
    "    style C fill:#e1f5e1,stroke:#333,color:#000\n",
    "    style C1 fill:#e1f5e1,stroke:#333,color:#000\n",
    "    style C2 fill:#e1f5e1,stroke:#333,color:#000\n",
    "    style C3 fill:#e1f5e1,stroke:#333,color:#000\n",
    "    style C4 fill:#e1f5e1,stroke:#333,color:#000\n",
    "```\n",
    "\n",
    "\n",
    "Как Tungsten работает:\n",
    "\n",
    "```python\n",
    "# ПРИМЕР: Операция length() для 16000 отзывов\n",
    "\n",
    "# Старый подход (до Tungsten):\n",
    "for review in reviews:           # Python-итератор\n",
    "    len(review)                  # Вызов функции Python\n",
    "    # Накладные расходы: упаковка/распаковка, проверка типов\n",
    "\n",
    "# Подход с Tungsten:\n",
    "# 1. Генерация кода для всего этапа выполнения:\n",
    "# Spark порождает простой байткод Java:\n",
    "for (int i = 0; i < 16000; i++) {\n",
    "    int length = utf8Length(data[i]);  # Прямой доступ к памяти\n",
    "    results[i] = length;               # Запись в двоичный буфер\n",
    "}\n",
    "\n",
    "# 2. Векторная обработка (если поддерживается процессором):\n",
    "# Векторные инструкции процессора (SIMD): обрабатываем 8 строк за одну операцию!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852a55ab",
   "metadata": {},
   "source": [
    "### 2.4. Catalyst Optimizer: Как это работает внутри\n",
    "\n",
    "### 2.1. Фазы оптимизации запроса\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│              ПРОЦЕСС ОПТИМИЗАЦИИ CATALYST               │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│ ФАЗА 1: Анализ                                          │\n",
    "│ • Разрешение имен таблиц и колонок                      │\n",
    "│ • Проверка типов данных                                 │\n",
    "│ • Валидация SQL-синтаксиса                              │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│ ФАЗА 2: Логическая оптимизация                          │\n",
    "│ • Predicate Pushdown (проталкивание предикатов)         │\n",
    "│ • Projection Pruning (отсечение ненужных колонок)       │\n",
    "│ • Constant Folding (сворачивание констант)              │\n",
    "│ • Filter Merging (объединение фильтров)                 │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│ ФАЗА 3: Физическое планирование                         │\n",
    "│ • Выбор алгоритмов соединения                           │\n",
    "│         (широковещательное хеш-соединение               │ \n",
    "│             против сортировочного соединения)           │\n",
    "│ • Оптимизация партиционирования                         │\n",
    "│ • Локальность данных (data locality)                    │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│ ФАЗА 4: Генерация кода                                  │\n",
    "│ • Генерация кода для всего этапа выполнения             │\n",
    "│ • Векторизованное выполнение                            │\n",
    "│ • Минимизация виртуальных вызовов                       │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aee9f71",
   "metadata": {},
   "source": [
    "### Сводная таблица отличий\n",
    "\n",
    "| Признак | Catalyst (Optimizer) | DAG Scheduler |\n",
    "| :--- | :--- | :--- |\n",
    "| Уровень | Высокоуровневый (Spark SQL/DataFrames) | Низкоуровневый (Spark Core, RDD) |\n",
    "| Основная цель | Оптимизация логики запроса | Планирование и выполнение задач на кластере |\n",
    "| Результат работы | Оптимизированный физический план выполнения (последовательность RDD операций) | DAG этапов (Stages) и управление задачами (Tasks) |\n",
    "| Ключевая концепция | Правила оптимизации (Predicate Pushdown, Join Selection) | Этапы (Stages), разделённые shuffle-границами |\n",
    "| Активность | Работает до выполнения, на этапе построения плана | Работает во время выполнения, управляет жизненным циклом задач |\n",
    "| Пользователь | \"Невидим\" для программиста, работает под капотом DataFrame API | Его логика отображается в Spark UI на вкладке \"Stages\" |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4bfad8",
   "metadata": {},
   "source": [
    "## 3. DataFrame API: Основные операции\n",
    "\n",
    "### 3.1. Иерархия операций DataFrame\n",
    "\n",
    "Разница: трансформации и действия\n",
    "\n",
    "- Трансформации (transformations)  \n",
    "  Описывают, *как* нужно преобразовать данные.  \n",
    "  Не запускают вычисления сразу (отложенные вычисления, *lazy evaluation*): Spark только строит план, но не читает и не обрабатывает данные до тех пор, пока не вызвано действие.\n",
    "\n",
    "- Действия (actions)  \n",
    "  Запускают реальное вычисление.  \n",
    "  Когда вы вызываете действие, Spark:\n",
    "  1) строит логический и физический план,  \n",
    "  2) оптимизирует его,  \n",
    "  3) выполняет все необходимые трансформации,  \n",
    "  4) возвращает результат в драйвер или записывает его во внешнее хранилище.\n",
    "\n",
    "---\n",
    "\n",
    "### Таблица операций DataFrame\n",
    "\n",
    "| Тип операции      | Подтип                  | Операция              | Что делает (кратко)                                                                 |\n",
    "|-------------------|-------------------------|-----------------------|-------------------------------------------------------------------------------------|\n",
    "| Трансформация | Проекции                | `select()`            | Выбирает подмножество колонок или выражений над колонками                           |\n",
    "|                   |                         | `selectExpr()`        | То же, что `select`, но выражения задаются строками в виде мини-SQL                 |\n",
    "|                   |                         | `withColumn()`        | Добавляет новую колонку или переопределяет существующую                             |\n",
    "| Трансформация | Фильтрация              | `filter()` / `where()`| Оставляет только строки, удовлетворяющие условию                                    |\n",
    "|                   |                         | `distinct()`          | Удаляет полностью дубликатные строки (по всем колонкам)                             |\n",
    "|                   |                         | `dropDuplicates()`    | Удаляет дубликаты по указанным колонкам                                             |\n",
    "| Трансформация | Агрегации (группировка) | `groupBy()`           | Группирует строки по ключу(ам) для последующих агрегатных функций                   |\n",
    "|                   |                         | `rollup()`            | Иерархическая агрегация (итоги по подуровням и общие итоги)                         |\n",
    "|                   |                         | `cube()`              | Многомерная агрегация по всем комбинациям измерений                                 |\n",
    "| Трансформация | Соединения              | `join()`              | Объединяет строки двух DataFrame по условию                                         |\n",
    "|                   |                         | `union()`             | Складывает строки двух совместимых по схеме DataFrame (как оператор UNION ALL)     |\n",
    "|                   |                         | `intersect()`         | Оставляет только те строки, которые есть в обоих DataFrame                          |\n",
    "| Трансформация | Оконные функции         | `window()`            | Определяет «окно» (рамку) по времени/ключам для оконных агрегатных функций          |\n",
    "|                   |                         | `rank()`              | Присваивает ранг в пределах окна (с возможными «дырами» в нумерации)                |\n",
    "|                   |                         | `row_number()`        | Нумерует строки подряд в пределах окна                                              |\n",
    "| Действие      | Сбор данных в драйвер   | `collect()`           | Вычисляет план и возвращает все данные в виде списка объектов в приложении         |\n",
    "|                   |                         | `show()`              | Вычисляет план и печатает первые N строк в консоль                                  |\n",
    "|                   |                         | `take()`              | Вычисляет план и возвращает первые N строк в виде списка                            |\n",
    "| Действие      | Агрегации как результат | `count()`             | Возвращает количество строк (число)                                                 |\n",
    "|                   |                         | `first()`             | Возвращает первую строку                                                            |\n",
    "|                   |                         | `reduce()`            | Сводит все элементы с помощью пользовательской функции                              |\n",
    "| Действие      | Запись                  | `write.csv()`         | Вычисляет план и сохраняет результат в файлы формата CSV                            |\n",
    "|                   |                         | `write.parquet()`     | Вычисляет план и сохраняет результат в формате Parquet                              |\n",
    "|                   |                         | `write.json()`        | Вычисляет план и сохраняет результат в формате JSON                                 |\n",
    "\n",
    "Кратко: трансформации описывают вычисление и возвращают новый DataFrame, а действия запускают вычисление и возвращают «конечный» результат (в память, на экран или в файловую систему)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e042990d",
   "metadata": {},
   "source": [
    "## 4. Инициализация Spark Session\n",
    "\n",
    "### 4.1. Архитектура Spark Application\n",
    "\n",
    "```\n",
    "Spark Application состоит из:\n",
    "• Driver Program (ваша программа)\n",
    "• SparkContext (точка входа для RDD)\n",
    "• SparkSession (точка входа для DataFrame)\n",
    "• Cluster Manager (YARN, Mesos, Standalone)\n",
    "• Executors (рабочие процессы на узлах)\n",
    "\n",
    "[ВАША ПРОГРАММА] → [DRIVER] → [CLUSTER MANAGER] → [EXECUTORS]\n",
    "       ↑                              ↓\n",
    "   Код на Python              Распределяет задачи\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02d7a3d",
   "metadata": {},
   "source": [
    "### Краткий справочник по настройкам (конфигурации) Spark\n",
    "\n",
    "#### Минимальный набор\n",
    "```python\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .master(\"local[*]\") \\          # Все ядра\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\  # Для маленьких данных\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\          # Память\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\ # Автооптимизация\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")  # Меньше логов\n",
    "```\n",
    "\n",
    "#### Память (OOM?)\n",
    "- `driver/executor.memory` ↑ при OutOfMemory\n",
    "- `memory.fraction=0.6` (по умолчанию) - память выполнения\n",
    "- `memory.storageFraction=0.5` - доля для кэша\n",
    "\n",
    "#### Оптимизации (всегда включать)\n",
    "- `adaptive.enabled=true` - автооптимизация shuffle\n",
    "- `autoBroadcastJoinThreshold=10MB` - маленькие таблицы в память\n",
    "- `files.maxPartitionBytes=128MB` - оптимальный размер партиции\n",
    "\n",
    "#### Для 16000 мелких файлов\n",
    "```python\n",
    ".config(\"spark.sql.files.openCostInBytes\", \"4MB\")  # Дешевое чтение файлов\n",
    ".config(\"spark.default.parallelism\", \"8\")          # Параллелизм по умолчанию\n",
    "```\n",
    "\n",
    "#### Правило партиций\n",
    "- 160MB данных → 4 партиции (shuffle.partitions=4)\n",
    "- 16GB данных → 200 партиций (по умолчанию)\n",
    "- >100GB данных → 400+ партиций\n",
    "\n",
    "#### Частые ошибки\n",
    "1. Медленный shuffle → уменьшить `shuffle.partitions`\n",
    "2. OutOfMemory → увеличить `*.memory`\n",
    "3. Мало задач → увеличить `default.parallelism`\n",
    "4. Медленные файлы → уменьшить `openCostInBytes`\n",
    "\n",
    "Итог для ETL: `adaptive=true`, `shuffle.partitions=4`, `memory=2g`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74eacd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка PySpark (если не установлен)\n",
    "# !pip install pyspark\n",
    "\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "import builtins\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d975ffa",
   "metadata": {},
   "source": [
    "### 4.2. Конфигурация для локальной разработки\n",
    "\n",
    "- `appName(\"Kinopoisk Reviews ETL\")`  \n",
    "  Имя приложения в Spark UI и логах, на работу не влияет.\n",
    "\n",
    "- `master(\"local\")`  \n",
    "  Локальный режим, использовать все ядра машины.\n",
    "\n",
    "- `spark.sql.shuffle.partitions = 4`  \n",
    "  Кол-во партиций при шафле (join/groupBy); уменьшено до 4 для маленьких локальных задач.\n",
    "\n",
    "- `spark.sql.adaptive.enabled = true`  \n",
    "  Включает адаптивный планировщик SQL: во время выполнения оптимизирует план (партиции, тип join).\n",
    "\n",
    "- `spark.driver.memory = 2g`  \n",
    "  Память JVM-процесса драйвера — 2 ГБ.\n",
    "\n",
    "- `spark.executor.memory = 2g`  \n",
    "  Память на каждый executor — 2 ГБ (в local фактически та же машина).\n",
    "\n",
    "- `spark.sparkContext.setLogLevel(\"WARN\")`  \n",
    "  Логировать только предупреждения и ошибки, скрыть INFO/DEBUG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71611ae",
   "metadata": {},
   "source": [
    "#### Инициализация SparkSession с оптимизациями "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f68139e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/19 15:26:07 WARN Utils: Your hostname, Mordor, resolves to a loopback address: 127.0.1.1; using 192.168.1.179 instead (on interface wlp0s20f3)\n",
      "25/12/19 15:26:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/19 15:26:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Kinopoisk_Reviews_ETL\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1057d6d",
   "metadata": {},
   "source": [
    "## 4. Источники данных в Spark\n",
    "\n",
    "### 4.1. Поддерживаемые форматы\n",
    "\n",
    "ТЕКСТОВЫЕ ФОРМАТЫ:\n",
    "- CSV, JSON, TXT — человекочитаемые, но относительно медленные и занимают больше места  \n",
    "- XML — иерархические данные (удобен для сложных вложенных структур)\n",
    "- Логи (форматы с разделителями, произвольный текст) — можно читать как текст и разбирать вручную\n",
    "\n",
    "БИНАРНЫЕ ФОРМАТЫ (предпочтительны для аналитики):\n",
    "- Parquet — колоночный, сжатый, хорошо подходит для аналитических запросов\n",
    "- ORC — колоночный формат, исторически тесно связан с Hive\n",
    "- Avro — поддерживает эволюцию схемы (изменение структуры данных без потери совместимости)\n",
    "- Delta Lake, Apache Iceberg, Apache Hudi — форматы «табличных данных на файловой системе» с поддержкой версионирования данных, транзакций и удобной работы с изменениями\n",
    "\n",
    "БАЗЫ ДАННЫХ И ХРАНИЛИЩА:\n",
    "- JDBC (PostgreSQL, MySQL, SQL Server и другие реляционные СУБД)\n",
    "- Cassandra, MongoDB — нереляционные (NoSQL) базы\n",
    "- HBase, Hive — распределённые хранилища поверх HDFS\n",
    "- ElasticSearch / OpenSearch — полнотекстовый поиск и аналитика по документам\n",
    "- Redis и другие ключ-значение хранилища (обычно как источник/кеш, а не основной дата-лейк)\n",
    "\n",
    "ПОТОКОВЫЕ ИСТОЧНИКИ:\n",
    "- Kafka — очереди сообщений и стриминг-системы\n",
    "- Файлы, которые появляются в каталогах (streaming чтение из папок)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056268e9",
   "metadata": {},
   "source": [
    "\n",
    "### 4.2. Проблема мелких файлов\n",
    "\n",
    "Пример структуры данных:\n",
    "\n",
    "reviews/  \n",
    "├── neg/ (19 804 файлов × 10 КБ ≈ 198 МБ)  \n",
    "├── pos/ (87 101 файл × 10 КБ ≈ 871 МБ)  \n",
    "└── neu/ (24 678 файлов × 10 КБ ≈ 247 МБ)  \n",
    "\n",
    "Всего: 131 583 файла, ≈1,3 ГБ данных\n",
    "\n",
    "В чём проблема:\n",
    "- Каждый файл → отдельная задача (task) в Spark  \n",
    "- Создание задачи занимает ~100 мс (накладные расходы)  \n",
    "- 131 583 × 0,1 с ≈ 13 000 секунд тратится только на подготовку задач  \n",
    "- Реальная обработка данных может занимать, например, всего ~1–2 минуты\n",
    "\n",
    "Как решать:\n",
    "- Объединять мелкие файлы (операции `coalesce` / `repartition`)  \n",
    "- Сохранять данные в колоночном формате (например, Parquet)  \n",
    "- Настраивать размер разделов (партиций), чтобы файлов было меньше и каждый был достаточного размера\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9de7f9a",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Чтение данных из сложной структуры\n",
    "\n",
    "### 5.1. Структура наших данных\n",
    "\n",
    "reviews/  \n",
    "├── neg/           # отрицательные отзывы  \n",
    "│   ├── XXX-XX.txt  \n",
    "│   ├── XXX-XX.txt  \n",
    "│   └── ...  \n",
    "├── pos/           # положительные отзывы  \n",
    "│   └── ...  \n",
    "└── neu/           # нейтральные отзывы  \n",
    "    └── ...\n",
    "\n",
    "Каждый файл:\n",
    "- Расширение: `*.txt`\n",
    "- Содержимое: текст отзыва\n",
    "- Метка класса: в имени папки (`neg` / `pos` / `neu`)\n",
    "\n",
    "Дополнительно:  \n",
    "Имя файла, например `306-15.txt`, может обозначать идентификатор фильма на сайте (`306`) и порядковый номер рецензии (`15`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff795fed",
   "metadata": {},
   "source": [
    "Практика\n",
    "\n",
    "#### Исследование структуры данных \n",
    "\n",
    "- Посмотреть структуру папки reviews/\n",
    "- Подсчитать количество файлов в каждой категории\n",
    "- Посмотреть примеры файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbe455ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Структура папок:\n",
      "dataset/ (0 файлов)\n",
      "  pos/ (87138 файлов)\n",
      "  neu/ (24704 файлов)\n",
      "  neg/ (19827 файлов)\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = \"reviews/archive/dataset\" \n",
    "\n",
    "if not os.path.exists(BASE_PATH):\n",
    "    print(f\"ОШИБКА: Папка {BASE_PATH} не найдена!\")\n",
    "else:\n",
    "    print(\"Структура папок:\")\n",
    "    for root, dirs, files in os.walk(BASE_PATH):\n",
    "        level = root.replace(BASE_PATH, '').count(os.sep)\n",
    "        indent = ' ' * 2 * level\n",
    "        print(f\"{indent}{os.path.basename(root)}/ ({len(files)} файлов)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a40249",
   "metadata": {},
   "source": [
    "#### Чтение текстовых файлов в DataFrame\n",
    "\n",
    "Прочитать все текстовые файлы и сохранить их в DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fe3f6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/19 15:26:10 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: reviews/archive/dataset/*/*.txt.\n",
      "java.io.FileNotFoundException: File reviews/archive/dataset/*/*.txt does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:980)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1301)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:970)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.sinks.FileStreamSink$.hasMetadata(FileStreamSink.scala:58)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:384)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:343)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:339)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:339)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:289)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:236)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:122)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:84)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:322)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:322)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:330)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:330)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:329)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:90)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:114)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:112)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:108)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:57)\n",
      "\tat org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:535)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.text(DataFrameReader.scala:328)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Схема DataFrame:\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Количество прочитанных отзывов: 131669\n",
      "\n",
      "Примеры данных:\n",
      "+--------------------------------------------------+\n",
      "|                                             value|\n",
      "+--------------------------------------------------+\n",
      "|”Чужого 3” зрители ждали долго. Они ждали его п...|\n",
      "|Люблю французское кино. Впрочем, я вообще кино ...|\n",
      "|I. Крылатый Хичкок:\\n\\nАльфред Хичкок (13.08.18...|\n",
      "|Помните, в детстве мы все...ну, не все, а те, к...|\n",
      "|Пересмешник. Североамериканский певчий пересмеш...|\n",
      "+--------------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# чтение всех текстовых файлов с сохранением пути\n",
    "raw_df = spark.read.text(f\"{BASE_PATH}/*/*.txt\", wholetext=True)\n",
    "\n",
    "# посмотрим, что получилось\n",
    "print(\"Схема DataFrame:\")\n",
    "raw_df.printSchema()\n",
    "\n",
    "print(f\"\\nКоличество прочитанных отзывов: {raw_df.count()}\")\n",
    "print(\"\\nПримеры данных:\")\n",
    "raw_df.show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2235c7e0",
   "metadata": {},
   "source": [
    "Проблема: \n",
    "мы потеряли информацию о тональности (neg/pos/neu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b284ae",
   "metadata": {},
   "source": [
    "### 5.3. Извлечение дополнительных метаданных\n",
    "\n",
    "Извлекаемые метаданные:\n",
    "1. Тональность (`neg` / `pos` / `neu`) — из пути\n",
    "2. Идентификатор фильма — из имени файла (первая часть, до дефиса)\n",
    "3. Номер рецензии — из имени файла (вторая часть, после дефиса)\n",
    "4. Дата создания файла — из метаданных файловой системы\n",
    "5. Размер файла — в байтах\n",
    "\n",
    "Пример имени файла: `306-15.txt`  \n",
    "- `306` — идентификатор фильма на сайте  \n",
    "- `15` — номер рецензии для этого фильма\n",
    "\n",
    "Регулярные выражения:\n",
    "- Путь:  \n",
    "  - `\".*/(neg|pos|neu)/.*\"`\n",
    "- Имя файла (выделяем id фильма и номер рецензии):  \n",
    "  - `\".*/(\\d+)-(\\d+)\\.txt\"`\n",
    "- Полный путь с разбором каталога и имени файла:  \n",
    "  - `\"(.*)/(neg|pos|neu)/(\\d+-\\d+\\.txt)\"`\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db80de8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Извлечение метаданных из путей и названия файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9b5a8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/19 15:26:48 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: reviews/archive/dataset/*/*.txt.\n",
      "java.io.FileNotFoundException: File reviews/archive/dataset/*/*.txt does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:980)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1301)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:970)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.sinks.FileStreamSink$.hasMetadata(FileStreamSink.scala:58)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:384)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:343)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:339)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:339)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:289)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:236)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:122)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:84)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:322)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:322)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:330)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:330)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:329)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:90)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:114)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:112)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:108)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:57)\n",
      "\tat org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:535)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.text(DataFrameReader.scala:328)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/12/19 15:26:59 WARN SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Схема DataFrame с метаданными:\n",
      "root\n",
      " |-- review_text: string (nullable = true)\n",
      " |-- file_path: string (nullable = false)\n",
      " |-- sentiment: string (nullable = false)\n",
      " |-- film_id: integer (nullable = true)\n",
      " |-- review_num: integer (nullable = true)\n",
      "\n",
      "\n",
      "Пример данных (первые 5 записей):\n",
      "+-------+----------+---------+----------------------------------------------------+\n",
      "|film_id|review_num|sentiment|text_preview                                        |\n",
      "+-------+----------+---------+----------------------------------------------------+\n",
      "|2286   |91        |pos      |”Чужого 3” зрители ждали долго. Они ждали его почт  |\n",
      "|20326  |3         |pos      |Люблю французское кино. Впрочем, я вообще кино люб  |\n",
      "|9118   |11        |neu      |I. Крылатый Хичкок:\\n\\nАльфред Хичкок (13.08.1899 – |\n",
      "|6044   |4         |pos      |Помните, в детстве мы все...ну, не все, а те, кто   |\n",
      "|357    |65        |pos      |Пересмешник. Североамериканский певчий пересмешник  |\n",
      "+-------+----------+---------+----------------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import input_file_name, regexp_extract, col, substring\n",
    "\n",
    "# читаем данные с wholetext=True (каждый файл = одна запись)\n",
    "raw_df = spark.read.text(f\"{BASE_PATH}/*/*.txt\", wholetext=True)\n",
    "\n",
    "# добавляем полный путь к файлу\n",
    "df_with_path = raw_df.withColumn(\"file_path\", input_file_name())\n",
    "\n",
    "# извлекаем ВСЕ метаданные из пути\n",
    "df_with_metadata = df_with_path.withColumn(\n",
    "    \"sentiment\",  # тональность: neg/pos/neu\n",
    "    regexp_extract(col(\"file_path\"), r\".*/(neg|pos|neu)/.*\", 1)\n",
    ").withColumn(\n",
    "    \"filename\",  # имя файла: 306-15.txt\n",
    "    regexp_extract(col(\"file_path\"), r\".*/([^/]+\\.txt)$\", 1)\n",
    ").withColumn(\n",
    "    \"film_id\",  # ID фильма: 306 (первая часть до дефиса)\n",
    "    regexp_extract(col(\"filename\"), r\"^(\\d+)-\\d+\\.txt$\", 1).cast(\"int\")\n",
    ").withColumn(\n",
    "    \"review_num\",  # номер рецензии: 15 (вторая часть после дефиса) \n",
    "    regexp_extract(col(\"filename\"), r\"^\\d+-(\\d+)\\.txt$\", 1).cast(\"int\")\n",
    ")\n",
    "\n",
    "# переименовываем колонку с текстом\n",
    "df = df_with_metadata.withColumnRenamed(\"value\", \"review_text\") \\\n",
    "    .drop(\"filename\")  # удаляем промежуточную колонку\n",
    "\n",
    "# проверяем результат\n",
    "print(\"Схема DataFrame с метаданными:\")\n",
    "df.printSchema()\n",
    "\n",
    "print(\"\\nПример данных (первые 5 записей):\")\n",
    "df.select(\"film_id\", \"review_num\", \"sentiment\", \n",
    "                substring(\"review_text\", 1, 50).alias(\"text_preview\")) \\\n",
    "        .show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e8564e",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Трансформации и очистка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b7121b",
   "metadata": {},
   "source": [
    "### 6.1. Типичные проблемы в текстовых данных\n",
    "\n",
    "```\n",
    "1. Пропуска:\n",
    "   • Пустые файлы\n",
    "   • Файлы только с пробелами\n",
    "   • NULL значения\n",
    "\n",
    "2. Аномалии:\n",
    "   • Слишком короткие отзывы (< 10 символов)\n",
    "   • Слишком длинные отзывы (> 5000 символов)\n",
    "   • Невалидная кодировка\n",
    "\n",
    "3. Структурнык проблемы:\n",
    "   • HTML-теги в тексте\n",
    "   • Спецсимволы\n",
    "   • Лишние пробелы, переносы строк\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c312cf27",
   "metadata": {},
   "source": [
    "### 1. Как Catalyst Optimizer оптимизирует наши проверки\n",
    "\n",
    "Все наши фильтры объединяются в один оптимизированный запрос.\n",
    "\n",
    "```python\n",
    "# Наш код:\n",
    "df_cleaned = df.filter(\n",
    "    (col(\"review_length\") >= 10) & \n",
    "    (col(\"review_length\") <= 5000) &\n",
    "    (col(\"sentiment\").isNotNull()) &\n",
    "    (col(\"film_id\") > 0)\n",
    ")\n",
    "\n",
    "# Catalyst делает:\n",
    "# 1. Filter Merging: объединяет все условия в один WHERE\n",
    "# 2. Predicate Pushdown: если данные в Parquet, фильтрует при чтении\n",
    "# 3. Constant Folding: вычисляет 10 <= review_length <= 5000 один раз\n",
    "\n",
    "# Итоговый план: Scan → Filter(все_условия_сразу) → Project\n",
    "```\n",
    "\n",
    "### 2. Векторизованные операции Tungsten для текста\n",
    "\n",
    "Почему length() и split() работают быстро:\n",
    "- Tungsten генерирует нативный байт-код\n",
    "- Обработка строк оптимизирована через sun.misc.Unsafe\n",
    "- Минимизация аллокаций объектов Python\n",
    "\n",
    "```python\n",
    "# Под капотом:\n",
    "# Наш код: length(col(\"review_text\"))\n",
    "# Tungsten: генерирует цикл с прямым доступом к памяти UTF-8 строк\n",
    "```\n",
    "\n",
    "### 3. Статистика в Spark SQL для определения границ\n",
    "\n",
    "```python\n",
    "# Мы используем эмпирические границы (10, 5000)\n",
    "# На практике можно использовать статистику Spark:\n",
    "\n",
    "# Spark собирает статистику при записи в Parquet\n",
    "# Можно использовать для оптимизации:\n",
    "# - min/max значения колонок\n",
    "# - количество null значений\n",
    "# - приблизительные квантили\n",
    "```\n",
    "\n",
    "### 4. Spark SQL vs DataFrame API для проверок\n",
    "\n",
    "```python\n",
    "# Оба подхода дают одинаковый оптимизированный план:\n",
    "\n",
    "# DataFrame API\n",
    "df.filter(col(\"review_length\") > 100)\n",
    "\n",
    "# Spark SQL  \n",
    "spark.sql(\"SELECT * FROM reviews WHERE review_length > 100\")\n",
    "\n",
    "# Catalyst создает одинаковое AST для обоих\n",
    "```\n",
    "\n",
    "### 5. Оптимизация группировок для проверки дубликатов\n",
    "\n",
    "```python\n",
    "# Наша проверка дубликатов:\n",
    "df.groupBy(\"film_id\", \"review_num\").count().filter(col(\"count\") > 1)\n",
    "\n",
    "# Catalyst оптимизирует:\n",
    "# 1. Использует hash-based агрегацию\n",
    "# 2. Применяет partial aggregation до shuffle\n",
    "# 3. Использует bloom filters для ускорения\n",
    "```\n",
    "\n",
    "Все наши проверки качества данных в DataFrame API автоматически оптимизируются Catalyst и выполняются эффективно благодаря Tungsten, даже если мы пишем их как простые цепочки фильтров."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9241b870",
   "metadata": {},
   "source": [
    "#### Обработка ошибок и очистка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a9f5ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проверка пропусков:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+---------+-------+----------+\n",
      "|review_text|file_path|sentiment|film_id|review_num|\n",
      "+-----------+---------+---------+-------+----------+\n",
      "|          0|        0|        0|      0|         0|\n",
      "+-----------+---------+---------+-------+----------+\n",
      "\n",
      "\n",
      "Проверка корректности числовых полей:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|bad_film_id|bad_review_num|\n",
      "+-----------+--------------+\n",
      "|          0|         15613|\n",
      "+-----------+--------------+\n",
      "\n",
      "\n",
      "Проверка дубликатов (film_id + review_num):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено дублирующихся пар film_id+review_num: 86\n",
      "\n",
      "Статистика по длине отзывов:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------------+------------------+\n",
      "|min_len|max_len|          avg_len|           std_len|\n",
      "+-------+-------+-----------------+------------------+\n",
      "|     58|  13642|2262.584852926657|1367.3904526812264|\n",
      "+-------+-------+-----------------+------------------+\n",
      "\n",
      "\n",
      "Статистика очистки:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "До очистки: 131669 записей\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "После очистки: 109998 записей\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Удалено: 21671 записей\n",
      "\n",
      "Анализ film_id по категориям тональности:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:==================================================>(4080 + 12) / 4115]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------------------+\n",
      "|sentiment|unique_films|    avg_review_len|\n",
      "+---------+------------+------------------+\n",
      "|      neg|        3641|2092.0192849014793|\n",
      "|      neu|        4868|2012.4027764103573|\n",
      "|      pos|        7876|2040.0542425363421|\n",
      "+---------+------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, count, length, split, size, countDistinct, avg, min, max, stddev\n",
    "\n",
    "# 1. проверяем пропуски по всем колонкам\n",
    "print(\"Проверка пропусков:\")\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "# 2. добавляем метрики качества\n",
    "df_enhanced = df.withColumn(\"review_length\", length(col(\"review_text\"))) \\\n",
    "                .withColumn(\"word_count\", size(split(col(\"review_text\"), \"\\\\s+\")))\n",
    "\n",
    "# 3. проверяем корректность film_id и review_num\n",
    "print(\"\\nПроверка корректности числовых полей:\")\n",
    "df_enhanced.select(\n",
    "    count(when(col(\"film_id\").isNull() | (col(\"film_id\") <= 0), True)).alias(\"bad_film_id\"),\n",
    "    count(when(col(\"review_num\").isNull() | (col(\"review_num\") <= 0), True)).alias(\"bad_review_num\")\n",
    ").show()\n",
    "\n",
    "# 4. проверяем дубликаты (уникальность film_id + review_num)\n",
    "print(\"\\nПроверка дубликатов (film_id + review_num):\")\n",
    "duplicate_count = df_enhanced.groupBy(\"film_id\", \"review_num\") \\\n",
    "    .count() \\\n",
    "    .filter(col(\"count\") > 1) \\\n",
    "    .count()\n",
    "print(f\"Найдено дублирующихся пар film_id+review_num: {duplicate_count}\")\n",
    "\n",
    "# 5. анализ распределения длины\n",
    "print(\"\\nСтатистика по длине отзывов:\")\n",
    "df_enhanced.select(\n",
    "    min(\"review_length\").alias(\"min_len\"),\n",
    "    max(\"review_length\").alias(\"max_len\"),\n",
    "    avg(\"review_length\").alias(\"avg_len\"),\n",
    "    stddev(\"review_length\").alias(\"std_len\")\n",
    ").show()\n",
    "\n",
    "# 6. расширенная фильтрация аномалий\n",
    "df_cleaned = df_enhanced.filter(\n",
    "    (col(\"review_length\") >= 10) & \n",
    "    (col(\"review_length\") <= 5000) &\n",
    "    (col(\"sentiment\").isNotNull()) &\n",
    "    (col(\"film_id\").isNotNull()) &\n",
    "    (col(\"film_id\") > 0) &\n",
    "    (col(\"review_num\").isNotNull()) &\n",
    "    (col(\"review_num\") > 0)\n",
    ")\n",
    "\n",
    "print(f\"\\nСтатистика очистки:\")\n",
    "print(f\"До очистки: {df_enhanced.count()} записей\")\n",
    "print(f\"После очистки: {df_cleaned.count()} записей\")\n",
    "print(f\"Удалено: {df_enhanced.count() - df_cleaned.count()} записей\")\n",
    "\n",
    "# 7. дополнительная проверка: анализ film_id по тональности\n",
    "print(\"\\nАнализ film_id по категориям тональности:\")\n",
    "df_cleaned.groupBy(\"sentiment\").agg(\n",
    "    countDistinct(\"film_id\").alias(\"unique_films\"),\n",
    "    avg(\"review_length\").alias(\"avg_review_len\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bbdc14",
   "metadata": {},
   "source": [
    "### 6.3. Анализ качества данных (EDA)\n",
    "\n",
    "```\n",
    "Метрика для анализа:\n",
    "1. Количество отзывов по тональности\n",
    "2. Распределение длины отзывов\n",
    "3. Средняя/медианная длина\n",
    "4. Стандартное отклонение\n",
    "5. Минимум/максимум\n",
    "\n",
    "Статистические границы:\n",
    "• Q1 - 1.5×IQR → нижняя граница выбросов\n",
    "• Q3 + 1.5×IQR → верхняя граница выбросов\n",
    "• IQR = Q3 - Q1 (интерквартильный размах)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364057d3",
   "metadata": {},
   "source": [
    "### 1. Статистические границы выбросов (IQR метод)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56bcba9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Статистические границы выбросов (IQR метод):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----+----+-----------+-----------+\n",
      "|sentiment|  q1|  q3| iqr|lower_bound|upper_bound|\n",
      "+---------+----+----+----+-----------+-----------+\n",
      "|      neu|1199|2669|1470|    -1006.0|     4874.0|\n",
      "|      neg|1296|2748|1452|     -882.0|     4926.0|\n",
      "|      pos|1236|2677|1441|     -925.5|     4838.5|\n",
      "+---------+----+----+----+-----------+-----------+\n",
      "\n",
      "\n",
      "Количество выбросов по IQR методу:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neu: 113 выбросов (границы: -1006-4874)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg: 65 выбросов (границы: -882-4926)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:==================================================>(4090 + 12) / 4115]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos: 523 выбросов (границы: -926-4838)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# квантили для каждой категории\n",
    "print(\"\\nСтатистические границы выбросов (IQR метод):\")\n",
    "iqr_stats = df_cleaned.groupBy(\"sentiment\").agg(\n",
    "    expr(\"percentile_approx(review_length, 0.25)\").alias(\"q1\"),\n",
    "    expr(\"percentile_approx(review_length, 0.75)\").alias(\"q3\")\n",
    ").withColumn(\"iqr\", col(\"q3\") - col(\"q1\")) \\\n",
    " .withColumn(\"lower_bound\", col(\"q1\") - 1.5 * col(\"iqr\")) \\\n",
    " .withColumn(\"upper_bound\", col(\"q3\") + 1.5 * col(\"iqr\"))\n",
    "\n",
    "iqr_stats.show()\n",
    "\n",
    "# подсчет выбросов по категориям\n",
    "print(\"\\nКоличество выбросов по IQR методу:\")\n",
    "for row in iqr_stats.collect():\n",
    "    sentiment = row[\"sentiment\"]\n",
    "    lower = row[\"lower_bound\"]\n",
    "    upper = row[\"upper_bound\"]\n",
    "    \n",
    "    outliers = df_cleaned.filter(\n",
    "        (col(\"sentiment\") == sentiment) & \n",
    "        ((col(\"review_length\") < lower) | (col(\"review_length\") > upper))\n",
    "    ).count()\n",
    "    \n",
    "    print(f\"{sentiment}: {outliers} выбросов (границы: {lower:.0f}-{upper:.0f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47f18b0",
   "metadata": {},
   "source": [
    "### 2. Анализ распределения film_id и review_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0451f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Анализ film_id (сколько отзывов на фильм):\n",
      "Топ-10 фильмов по количеству отзывов:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------------------+\n",
      "|film_id|review_count|        avg_length|\n",
      "+-------+------------+------------------+\n",
      "|1048334|         104| 2482.769230769231|\n",
      "| 481086|          99| 2388.969696969697|\n",
      "| 406141|          99|1373.4141414141413|\n",
      "|  77454|          99|1722.8080808080808|\n",
      "|   2950|          99| 2112.626262626263|\n",
      "|   8219|          99|1837.3232323232323|\n",
      "| 279627|          99|1610.1515151515152|\n",
      "| 493768|          99|1987.2424242424242|\n",
      "| 396193|          98| 2064.816326530612|\n",
      "|  47814|          98| 1875.795918367347|\n",
      "+-------+------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Статистика по review_num (проверка целостности нумерации):\n",
      "Фильмы с пропущенными номерами рецензий:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:==================================================>(4072 + 12) / 4115]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+------------+--------------+---------------+\n",
      "|film_id|min_num|max_num|actual_count|expected_count|missing_reviews|\n",
      "+-------+-------+-------+------------+--------------+---------------+\n",
      "| 279600|      1|     99|          95|            99|              4|\n",
      "|1114927|      1|     99|          92|            99|              7|\n",
      "|   2962|      1|     63|          61|            63|              2|\n",
      "| 647676|      1|     34|          33|            34|              1|\n",
      "|  77540|      1|     99|          90|            99|              9|\n",
      "|   6241|      1|     23|          22|            23|              1|\n",
      "| 405608|      1|     99|          89|            99|             10|\n",
      "| 500617|      1|     99|          86|            99|             13|\n",
      "| 397220|      1|     45|          43|            45|              2|\n",
      "| 542581|      1|     99|          95|            99|              4|\n",
      "+-------+-------+-------+------------+--------------+---------------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"\\nАнализ film_id (сколько отзывов на фильм):\")\n",
    "film_stats = df_cleaned.groupBy(\"film_id\").agg(\n",
    "    count(\"*\").alias(\"review_count\"),\n",
    "    avg(\"review_length\").alias(\"avg_length\")\n",
    ").orderBy(col(\"review_count\").desc())\n",
    "\n",
    "print(\"Топ-10 фильмов по количеству отзывов:\")\n",
    "film_stats.show(10)\n",
    "\n",
    "print(\"\\nСтатистика по review_num (проверка целостности нумерации):\")\n",
    "review_num_stats = df_cleaned.groupBy(\"film_id\").agg(\n",
    "    min(\"review_num\").alias(\"min_num\"),\n",
    "    max(\"review_num\").alias(\"max_num\"),\n",
    "    count(\"*\").alias(\"actual_count\"),\n",
    "    (max(\"review_num\") - min(\"review_num\") + 1).alias(\"expected_count\")\n",
    ").withColumn(\"missing_reviews\", col(\"expected_count\") - col(\"actual_count\"))\n",
    "\n",
    "print(\"Фильмы с пропущенными номерами рецензий:\")\n",
    "review_num_stats.filter(col(\"missing_reviews\") > 0).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8e2796",
   "metadata": {},
   "source": [
    "### 3. Анализ корреляции между длиной и тональностью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d982de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Корреляция длины отзыва и тональности:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 63:==================================================>(4105 + 10) / 4115]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Корреляция между тональностью и длиной отзыва: -0.012\n",
      "Слабая корреляция: длина не зависит от тональности\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "\n",
    "print(\"\\nКорреляция длины отзыва и тональности:\")\n",
    "# преобразуем тональность в числовой формат для корреляции\n",
    "df_numeric = df_cleaned.withColumn(\n",
    "    \"sentiment_num\", \n",
    "    when(col(\"sentiment\") == \"neg\", 1)\n",
    "    .when(col(\"sentiment\") == \"neu\", 2)\n",
    "    .when(col(\"sentiment\") == \"pos\", 3)\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "correlation = df_numeric.select(\n",
    "    corr(\"sentiment_num\", \"review_length\").alias(\"correlation\")\n",
    ").first()[0]\n",
    "\n",
    "print(f\"Корреляция между тональностью и длиной отзыва: {correlation:.3f}\")\n",
    "\n",
    "# интерпретация:\n",
    "if correlation > 0.3:\n",
    "    print(\"Заметная положительная корреляция: позитивные отзывы длиннее\")\n",
    "elif correlation < -0.3:\n",
    "    print(\"Заметная отрицательная корреляция: негативные отзывы длиннее\")\n",
    "else:\n",
    "    print(\"Слабая корреляция: длина не зависит от тональности\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4e46d8",
   "metadata": {},
   "source": [
    "### 4. Анализ частоты слов по тональности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37b88823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Анализ частоты слов по тональностям (топ-10):\n",
      "\n",
      "Топ-10 слов для pos:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|word   |count |\n",
      "+-------+------+\n",
      "|фильм  |110927|\n",
      "|очень  |85295 |\n",
      "|только |55886 |\n",
      "|просто |51530 |\n",
      "|даже   |51433 |\n",
      "|этот   |50823 |\n",
      "|если   |45758 |\n",
      "|который|41313 |\n",
      "|когда  |40761 |\n",
      "|фильма |38620 |\n",
      "+-------+------+\n",
      "\n",
      "\n",
      "Топ-10 слов для neg:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|word  |count|\n",
      "+------+-----+\n",
      "|фильм |26560|\n",
      "|даже  |15791|\n",
      "|только|15344|\n",
      "|если  |15074|\n",
      "|просто|14430|\n",
      "|было  |13865|\n",
      "|очень |13269|\n",
      "|фильма|11570|\n",
      "|можно |11197|\n",
      "|этот  |10972|\n",
      "+------+-----+\n",
      "\n",
      "\n",
      "Топ-10 слов для neu:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 72:==================================================>(4096 + 12) / 4115]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|word  |count|\n",
      "+------+-----+\n",
      "|фильм |31143|\n",
      "|очень |19027|\n",
      "|только|15679|\n",
      "|если  |15552|\n",
      "|даже  |15260|\n",
      "|было  |13448|\n",
      "|просто|12980|\n",
      "|фильма|12238|\n",
      "|можно |12235|\n",
      "|этот  |11120|\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, split, lower, length, col\n",
    "\n",
    "print(\"\\nАнализ частоты слов по тональностям (топ-10):\")\n",
    "\n",
    "# разбиваем текст на слова и анализируем\n",
    "words_df = df_cleaned.select(\n",
    "    \"sentiment\",\n",
    "    explode(split(lower(col(\"review_text\")), \"\\\\s+\")).alias(\"word\")\n",
    ").filter(\n",
    "    (length(col(\"word\")) > 3) &  # игнорируем короткие слова\n",
    "    (~col(\"word\").rlike(r\"\\d+\"))  # игнорируем числа\n",
    ")\n",
    "\n",
    "# топ слова для каждой тональности\n",
    "for sentiment in [\"pos\", \"neg\", \"neu\"]:\n",
    "    print(f\"\\nТоп-10 слов для {sentiment}:\")\n",
    "    words_df.filter(col(\"sentiment\") == sentiment) \\\n",
    "        .groupBy(\"word\") \\\n",
    "        .count() \\\n",
    "        .orderBy(col(\"count\").desc()) \\\n",
    "        .limit(10) \\\n",
    "        .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d481dfbd",
   "metadata": {},
   "source": [
    "### 5. Визуализация распределения через квантили\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b368766b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Распределение длин через квантили:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POS - квантили длины:\n",
      "  10%: 867  |  25%: 1250  |  50%: 1828\n",
      "  75%: 2692  |  90%: 3571  |  95%: 4119\n",
      "  99%: 5000\n",
      "  Внимание: 1% отзывов длиннее 5000 символов\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NEG - квантили длины:\n",
      "  10%: 861  |  25%: 1298  |  50%: 1903\n",
      "  75%: 2707  |  90%: 3684  |  95%: 4034\n",
      "  99%: 5000\n",
      "  Внимание: 1% отзывов длиннее 5000 символов\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 81:==================================================>(4064 + 12) / 4115]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NEU - квантили длины:\n",
      "  10%: 725  |  25%: 1176  |  50%: 1799\n",
      "  75%: 2673  |  90%: 3564  |  95%: 4080\n",
      "  99%: 5000\n",
      "  Внимание: 1% отзывов длиннее 5000 символов\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"\\nРаспределение длин через квантили:\")\n",
    "for sentiment in [\"pos\", \"neg\", \"neu\"]:\n",
    "    subset = df_cleaned.filter(col(\"sentiment\") == sentiment)\n",
    "    quantiles = subset.approxQuantile(\n",
    "        \"review_length\", \n",
    "        [0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99], \n",
    "        0.01\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{sentiment.upper()} - квантили длины:\")\n",
    "    print(f\"  10%: {quantiles[0]:.0f}  |  25%: {quantiles[1]:.0f}  |  50%: {quantiles[2]:.0f}\")\n",
    "    print(f\"  75%: {quantiles[3]:.0f}  |  90%: {quantiles[4]:.0f}  |  95%: {quantiles[5]:.0f}\")\n",
    "    print(f\"  99%: {quantiles[6]:.0f}\")\n",
    "\n",
    "    if quantiles[6] > 3000:\n",
    "        print(f\"  Внимание: 1% отзывов длиннее {quantiles[6]:.0f} символов\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c780b20a",
   "metadata": {},
   "source": [
    "#### Разведочный анализ данных (EDA) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c16a3d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статистика по категориям тональности:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+------------------+------------------+----------+----------+-------------+\n",
      "|sentiment|count_reviews|        avg_length|        std_length|min_length|max_length|median_length|\n",
      "+---------+-------------+------------------+------------------+----------+----------+-------------+\n",
      "|      pos|        72987|2040.0542425363421| 1051.288871516491|        62|      5000|         1837|\n",
      "|      neu|        20314|2012.4027764103573|1072.9284107579115|        58|      5000|         1824|\n",
      "|      neg|        16697|2092.0192849014793|1045.0218599134996|        98|      5000|         1902|\n",
      "+---------+-------------+------------------+------------------+----------+----------+-------------+\n",
      "\n",
      "\n",
      "Топ-3 самых длинных отзывов по категориям:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-------------+----------------------------------------------------------------------------------------------------+\n",
      "|sentiment|rank|review_length|preview                                                                                             |\n",
      "+---------+----+-------------+----------------------------------------------------------------------------------------------------+\n",
      "|neg      |1   |5000         |'Богопротивная, дрянная вещь тоска...' когда-то жаловался Верлен. Фильм Меркуловой и Чупова про чело|\n",
      "|neg      |2   |4996         |Шёл год 1997. Почти – конец века, и практически – начало новой эры развлекательного кинематографа. К|\n",
      "|neg      |3   |4995         |Месть — вредящие действия, произведенные из побуждения покарать за реальную или мнимую несправедливо|\n",
      "|neu      |1   |5000         |Шедевры о прочности веры появляются на экране крайне редко, в лучшем случае один раз в десятилетие, |\n",
      "|neu      |2   |4998         |90-ые… сколько же воспоминаний накрывает, когда ты начинаешь задумываться и вспоминать о них. Для ме|\n",
      "|neu      |3   |4997         |Мистический триллер на тему возможного всеобщего апокалипсиса в рядах человеческих, на тему свершивш|\n",
      "|pos      |1   |5000         |Когда говорят о правосудии, обычно подразумевают справедливость, т. е. виновные должны быть  наказан|\n",
      "|pos      |2   |5000         |Вот я и посмотрел вторую по счету картину бельгийского режиссера Жако Ван Дормеля, которая имеет наз|\n",
      "|pos      |3   |5000         |Итак, в фильме – начало 80-х. Движение хиппи и антивоенные протесты закончились, негры уже получили |\n",
      "+---------+----+-------------+----------------------------------------------------------------------------------------------------+\n",
      "\n",
      "\n",
      "Гистограмма распределения длин (по категориям):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POS (всего 72987 отзывов):\n",
      "  62- 555 chars: ####### (2368 reviews)\n",
      " 555-1049 chars: ################################# (10239 reviews)\n",
      "1049-1543 chars: ################################################## (15283 reviews)\n",
      "1543-2037 chars: ############################################ (13707 reviews)\n",
      "2037-2531 chars: ################################## (10595 reviews)\n",
      "2531-3024 chars: ######################## (7590 reviews)\n",
      "3024-3518 chars: ################ (5129 reviews)\n",
      "3518-4012 chars: ############ (3675 reviews)\n",
      "4012-4506 chars: ######## (2539 reviews)\n",
      "4506-5000 chars: ###### (1862 reviews)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NEG (всего 16697 отзывов):\n",
      "  98- 588 chars: ####### (537 reviews)\n",
      " 588-1078 chars: ############################### (2198 reviews)\n",
      "1078-1568 chars: ################################################## (3438 reviews)\n",
      "1568-2058 chars: ############################################# (3107 reviews)\n",
      "2058-2549 chars: ################################### (2428 reviews)\n",
      "2549-3039 chars: ########################## (1819 reviews)\n",
      "3039-3529 chars: ################## (1268 reviews)\n",
      "3529-4019 chars: ############ (893 reviews)\n",
      "4019-4509 chars: ######## (586 reviews)\n",
      "4509-5000 chars: ###### (423 reviews)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 94:==================================================>(4041 + 12) / 4115]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NEU (всего 20314 отзывов):\n",
      "  58- 552 chars: ############ (1034 reviews)\n",
      " 552-1046 chars: ################################## (2816 reviews)\n",
      "1046-1540 chars: ################################################## (4071 reviews)\n",
      "1540-2034 chars: ############################################# (3741 reviews)\n",
      "2034-2529 chars: ################################### (2898 reviews)\n",
      "2529-3023 chars: ######################### (2088 reviews)\n",
      "3023-3517 chars: ################# (1427 reviews)\n",
      "3517-4011 chars: ############ (1028 reviews)\n",
      "4011-4505 chars: ######## (695 reviews)\n",
      "4505-5000 chars: ###### (516 reviews)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 1. базовая статистика по категориям\n",
    "print(\"Статистика по категориям тональности:\")\n",
    "summary_df = df_cleaned.groupBy(\"sentiment\").agg(\n",
    "    count(\"*\").alias(\"count_reviews\"),\n",
    "    avg(\"review_length\").alias(\"avg_length\"),\n",
    "    stddev(\"review_length\").alias(\"std_length\"),\n",
    "    min(\"review_length\").alias(\"min_length\"),\n",
    "    max(\"review_length\").alias(\"max_length\"),\n",
    "    expr(\"percentile_approx(review_length, 0.5)\").alias(\"median_length\")\n",
    ").orderBy(\"count_reviews\", ascending=False)\n",
    "\n",
    "summary_df.show()\n",
    "\n",
    "# 2. топ-5 самых длинных отзывов по категориям\n",
    "window_spec = Window.partitionBy(\"sentiment\").orderBy(col(\"review_length\").desc())\n",
    "\n",
    "df_with_rank = df_cleaned.withColumn(\"rank\", row_number().over(window_spec))\n",
    "\n",
    "print(\"\\nТоп-3 самых длинных отзывов по категориям:\")\n",
    "top_reviews = df_with_rank.filter(col(\"rank\") <= 3) \\\n",
    "                         .select(\"sentiment\", \"rank\", \"review_length\", \n",
    "                                 substring(\"review_text\", 1, 100).alias(\"preview\"))\n",
    "top_reviews.show(truncate=False)\n",
    "\n",
    "# 3. гистограмма длин (псевдографика)\n",
    "print(\"\\nГистограмма распределения длин (по категориям):\")\n",
    "\n",
    "\n",
    "\n",
    "# сначала соберем все данные для гистограммы\n",
    "for sentiment in [\"pos\", \"neg\", \"neu\"]:\n",
    "    subset = df_cleaned.filter(col(\"sentiment\") == sentiment)\n",
    "    \n",
    "    # собираем длины в Python список\n",
    "    lengths = [row.review_length for row in subset.select(\"review_length\").collect()]\n",
    "    \n",
    "    if lengths:  # проверяем, что список не пустой\n",
    "        # используем builtins.min/max вместо переопределенных PySpark\n",
    "        min_len = builtins.min(lengths)\n",
    "        max_len = builtins.max(lengths)\n",
    "        \n",
    "        # создаем bins вручную\n",
    "        num_bins = 10\n",
    "        bin_width = (max_len - min_len) / num_bins\n",
    "        \n",
    "        bins = [min_len + i * bin_width for i in range(num_bins + 1)]\n",
    "        hist = [0] * num_bins\n",
    "        \n",
    "        for length in lengths:\n",
    "            for i in range(num_bins):\n",
    "                if bins[i] <= length < bins[i+1]:\n",
    "                    hist[i] += 1\n",
    "                    break\n",
    "            else:\n",
    "                # для последнего значения (включая границу)\n",
    "                if length == bins[-1]:\n",
    "                    hist[-1] += 1\n",
    "        \n",
    "        print(f\"\\n{sentiment.upper()} (всего {len(lengths)} отзывов):\")\n",
    "        \n",
    "        # находим максимум для нормализации\n",
    "        max_count = builtins.max(hist) if hist else 1\n",
    "        \n",
    "        for i in range(num_bins):\n",
    "            bar_length = int(hist[i] / max_count * 50) if max_count > 0 else 0\n",
    "            bar = \"#\" * bar_length\n",
    "            print(f\"{int(bins[i]):4d}-{int(bins[i+1]):4d} chars: {bar} ({hist[i]} reviews)\")\n",
    "    else:\n",
    "        print(f\"\\n{sentiment.upper()}: нет данных\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a84c8a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### 7.1. Как посмотреть план выполнения\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Наш запрос<br/>filter + groupBy + agg] --> B\n",
    "    \n",
    "    subgraph B [Catalyst Optimizer]\n",
    "        B1[Анализ запроса] --> B2[Логическая оптимизация]\n",
    "        B2 --> B3[Физическое планирование]\n",
    "    end\n",
    "    \n",
    "    B --> C\n",
    "    \n",
    "    subgraph C [Оптимизации для нашего ETL]\n",
    "        C1[Объединение фильтров<br/>sentiment='pos' & length>100]\n",
    "        C2[Отсечение колонок<br/>Только нужные поля]\n",
    "        C3[Проталкивание предикатов<br/>в Parquet reader]\n",
    "    end\n",
    "    \n",
    "    C --> D[Tungsten Engine]\n",
    "    \n",
    "    D --> E[Code Generation<br/>Быстрые примитивы]\n",
    "    D --> F[Векторизация<br/>Обработка батчами]\n",
    "    \n",
    "    E --> G[Результат]\n",
    "    F --> G\n",
    "    \n",
    "    style B fill:#e1f5fe,color:#000000\n",
    "    style C fill:#d4edda,color:#000000\n",
    "    style D fill:#fff3cd,color:#000000\n",
    "```\n",
    "\n",
    "\n",
    "### 7.2. Типы планов выполнения\n",
    "\n",
    "```\n",
    "1. Логический план:\n",
    "   • Описывает что нужно сделать\n",
    "   • Не учитывает распределенность\n",
    "   • Пример: Scan → Filter → Project → Aggregate\n",
    "\n",
    "2. Физический план:\n",
    "   • Описывает как это сделать\n",
    "   • Учитывает распределенность, shuffle\n",
    "   • Пример: Scan → Filter → HashAggregate → Exchange → HashAggregate\n",
    "\n",
    "3. Оптимизированный логический план:\n",
    "   • После применения правил оптимизации\n",
    "   • Predicate Pushdown, Projection Pruning, Constant Folding\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46dee30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сравнение: DataFrame API и Ручная оптимизация\n",
      "\n",
      "НЕОПТИМИЗИРОВАННЫЙ подход:\n",
      "План выполнения (плохой):\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[sentiment#15], functions=[count(1), avg(review_length#74)])\n",
      "   +- Exchange hashpartitioning(sentiment#15, 4), ENSURE_REQUIREMENTS, [plan_id=1981]\n",
      "      +- HashAggregate(keys=[sentiment#15], functions=[partial_count(1), partial_avg(review_length#74)])\n",
      "         +- Project [regexp_extract(file_path#14, .*/(neg|pos|neu)/.*, 1) AS sentiment#15, length(value#12) AS review_length#74]\n",
      "            +- Filter (isnotnull(value#12) AND (((((((length(value#12) >= 10) AND (length(value#12) <= 5000)) AND isnotnull(cast(regexp_extract(regexp_extract(file_path#14, .*/([^/]+\\.txt)$, 1), ^(\\d+)-\\d+\\.txt$, 1) as int))) AND (cast(regexp_extract(regexp_extract(file_path#14, .*/([^/]+\\.txt)$, 1), ^(\\d+)-\\d+\\.txt$, 1) as int) > 0)) AND isnotnull(cast(regexp_extract(regexp_extract(file_path#14, .*/([^/]+\\.txt)$, 1), ^\\d+-(\\d+)\\.txt$, 1) as int))) AND (cast(regexp_extract(regexp_extract(file_path#14, .*/([^/]+\\.txt)$, 1), ^\\d+-(\\d+)\\.txt$, 1) as int) > 0)) AND ((regexp_extract(file_path#14, .*/(neg|pos|neu)/.*, 1) = pos) AND (length(value#12) > 100))))\n",
      "               +- Project [value#12, input_file_name() AS file_path#14]\n",
      "                  +- FileScan text [value#12] Batched: false, DataFilters: [], Format: Text, Location: InMemoryFileIndex(131669 paths)[file:/home/shoose/Документы/reviews/archive/dataset/neg/1000083-0..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "\n",
      "\n",
      "\n",
      "ОПТИМИЗИРОВАННЫЙ подход (наш ETL):\n",
      "План выполнения (хороший):\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[sentiment#15], functions=[count(1), avg(review_length#74)])\n",
      "   +- Exchange hashpartitioning(sentiment#15, 4), ENSURE_REQUIREMENTS, [plan_id=2004]\n",
      "      +- HashAggregate(keys=[sentiment#15], functions=[partial_count(1), partial_avg(review_length#74)])\n",
      "         +- Project [regexp_extract(file_path#14, .*/(neg|pos|neu)/.*, 1) AS sentiment#15, length(value#12) AS review_length#74]\n",
      "            +- Filter (isnotnull(value#12) AND (((((((length(value#12) >= 10) AND (length(value#12) <= 5000)) AND isnotnull(cast(regexp_extract(regexp_extract(file_path#14, .*/([^/]+\\.txt)$, 1), ^(\\d+)-\\d+\\.txt$, 1) as int))) AND (cast(regexp_extract(regexp_extract(file_path#14, .*/([^/]+\\.txt)$, 1), ^(\\d+)-\\d+\\.txt$, 1) as int) > 0)) AND isnotnull(cast(regexp_extract(regexp_extract(file_path#14, .*/([^/]+\\.txt)$, 1), ^\\d+-(\\d+)\\.txt$, 1) as int))) AND (cast(regexp_extract(regexp_extract(file_path#14, .*/([^/]+\\.txt)$, 1), ^\\d+-(\\d+)\\.txt$, 1) as int) > 0)) AND ((regexp_extract(file_path#14, .*/(neg|pos|neu)/.*, 1) = pos) AND (length(value#12) > 100))))\n",
      "               +- Project [value#12, input_file_name() AS file_path#14]\n",
      "                  +- FileScan text [value#12] Batched: false, DataFilters: [], Format: Text, Location: InMemoryFileIndex(131669 paths)[file:/home/shoose/Документы/reviews/archive/dataset/neg/1000083-0..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, avg\n",
    "\n",
    "# Покажем разницу между неоптимизированным и оптимизированным запросом\n",
    "print(\"Сравнение: DataFrame API и Ручная оптимизация\")\n",
    "\n",
    "# неоптимизированный подход (частая ошибка)\n",
    "print(\"\\nНЕОПТИМИЗИРОВАННЫЙ подход:\")\n",
    "bad_query = df_cleaned \\\n",
    "    .select(\"*\") \\\n",
    "    .filter(col(\"sentiment\") == \"pos\") \\\n",
    "    .select(\"sentiment\", \"review_length\") \\\n",
    "    .filter(col(\"review_length\") > 100) \\\n",
    "    .groupBy(\"sentiment\") \\\n",
    "    .agg(count(\"*\").alias(\"count\"), avg(\"review_length\").alias(\"avg_len\"))\n",
    "\n",
    "print(\"План выполнения (плохой):\")\n",
    "bad_query.explain(\"simple\")\n",
    "\n",
    "# оптимизированный подход (наш)\n",
    "print(\"\\nОПТИМИЗИРОВАННЫЙ подход (наш ETL):\")\n",
    "good_query = df_cleaned \\\n",
    "    .filter((col(\"sentiment\") == \"pos\") & (col(\"review_length\") > 100)) \\\n",
    "    .groupBy(\"sentiment\") \\\n",
    "    .agg(count(\"*\").alias(\"count\"), avg(\"review_length\").alias(\"avg_len\"))\n",
    "\n",
    "print(\"План выполнения (хороший):\")\n",
    "good_query.explain(\"simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c936dc3",
   "metadata": {},
   "source": [
    "#### Оптимизация через Catalyst Optimizer  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81953622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "План выполнения запроса\n",
      "\n",
      "Форматированный план:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (8)\n",
      "+- ObjectHashAggregate (7)\n",
      "   +- Exchange (6)\n",
      "      +- ObjectHashAggregate (5)\n",
      "         +- Project (4)\n",
      "            +- Filter (3)\n",
      "               +- Project (2)\n",
      "                  +- Scan text  (1)\n",
      "\n",
      "\n",
      "(1) Scan text \n",
      "Output [1]: [value#12]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/home/shoose/Документы/reviews/archive/dataset/neg/1000083-0.txt, ... 131668 entries]\n",
      "ReadSchema: struct<value:string>\n",
      "\n",
      "(2) Project\n",
      "Output [2]: [value#12, input_file_name() AS file_path#14]\n",
      "Input [1]: [value#12]\n",
      "\n",
      "(3) Filter\n",
      "Input [2]: [value#12, file_path#14]\n",
      "Condition : (isnotnull(value#12) AND (((((((length(value#12) >= 10) AND (length(value#12) <= 5000)) AND isnotnull(cast(regexp_extract(regexp_extract(file_path#14, .*/([^/]+\\.txt)$, 1), ^(\\d+)-\\d+\\.txt$, 1) as int))) AND (cast(regexp_extract(regexp_extract(file_path#14, .*/([^/]+\\.txt)$, 1), ^(\\d+)-\\d+\\.txt$, 1) as int) > 0)) AND isnotnull(cast(regexp_extract(regexp_extract(file_path#14, .*/([^/]+\\.txt)$, 1), ^\\d+-(\\d+)\\.txt$, 1) as int))) AND (cast(regexp_extract(regexp_extract(file_path#14, .*/([^/]+\\.txt)$, 1), ^\\d+-(\\d+)\\.txt$, 1) as int) > 0)) AND ((regexp_extract(file_path#14, .*/(neg|pos|neu)/.*, 1) = pos) AND (length(value#12) > 100))))\n",
      "\n",
      "(4) Project\n",
      "Output [2]: [regexp_extract(file_path#14, .*/(neg|pos|neu)/.*, 1) AS sentiment#15, length(value#12) AS review_length#74]\n",
      "Input [2]: [value#12, file_path#14]\n",
      "\n",
      "(5) ObjectHashAggregate\n",
      "Input [2]: [sentiment#15, review_length#74]\n",
      "Keys [1]: [sentiment#15]\n",
      "Functions [3]: [partial_count(1), partial_avg(review_length#74), partial_percentile_approx(review_length#74, 0.9, 10000, 0, 0)]\n",
      "Aggregate Attributes [4]: [count#125398L, sum#125399, count#125400L, buf#125401]\n",
      "Results [5]: [sentiment#15, count#125402L, sum#125403, count#125404L, buf#125405]\n",
      "\n",
      "(6) Exchange\n",
      "Input [5]: [sentiment#15, count#125402L, sum#125403, count#125404L, buf#125405]\n",
      "Arguments: hashpartitioning(sentiment#15, 4), ENSURE_REQUIREMENTS, [plan_id=2027]\n",
      "\n",
      "(7) ObjectHashAggregate\n",
      "Input [5]: [sentiment#15, count#125402L, sum#125403, count#125404L, buf#125405]\n",
      "Keys [1]: [sentiment#15]\n",
      "Functions [3]: [count(1), avg(review_length#74), percentile_approx(review_length#74, 0.9, 10000, 0, 0)]\n",
      "Aggregate Attributes [3]: [count(1)#125395L, avg(review_length#74)#125396, percentile_approx(review_length#74, 0.9, 10000, 0, 0)#125397]\n",
      "Results [4]: [sentiment#15, count(1)#125395L AS count#125385L, avg(review_length#74)#125396 AS avg_len#125386, percentile_approx(review_length#74, 0.9, 10000, 0, 0)#125397 AS p90_len#125387]\n",
      "\n",
      "(8) AdaptiveSparkPlan\n",
      "Output [4]: [sentiment#15, count#125385L, avg_len#125386, p90_len#125387]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "\n",
      "Сравнение с SQL\n",
      "SQL план выполнения:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- ObjectHashAggregate(keys=[sentiment#15], functions=[count(1), avg(review_length#74), percentile_approx(review_length#74, 0.9, 10000, 0, 0)])\n",
      "   +- Exchange hashpartitioning(sentiment#15, 4), ENSURE_REQUIREMENTS, [plan_id=2053]\n",
      "      +- ObjectHashAggregate(keys=[sentiment#15], functions=[partial_count(1), partial_avg(review_length#74), partial_percentile_approx(review_length#74, 0.9, 10000, 0, 0)])\n",
      "         +- Project [regexp_extract(file_path#14, .*/(neg|pos|neu)/.*, 1) AS sentiment#15, length(value#12) AS review_length#74]\n",
      "            +- Filter (isnotnull(value#12) AND (((((((length(value#12) >= 10) AND (length(value#12) <= 5000)) AND isnotnull(cast(regexp_extract(regexp_extract(file_path#14, .*/([^/]+\\.txt)$, 1), ^(\\d+)-\\d+\\.txt$, 1) as int))) AND (cast(regexp_extract(regexp_extract(file_path#14, .*/([^/]+\\.txt)$, 1), ^(\\d+)-\\d+\\.txt$, 1) as int) > 0)) AND isnotnull(cast(regexp_extract(regexp_extract(file_path#14, .*/([^/]+\\.txt)$, 1), ^\\d+-(\\d+)\\.txt$, 1) as int))) AND (cast(regexp_extract(regexp_extract(file_path#14, .*/([^/]+\\.txt)$, 1), ^\\d+-(\\d+)\\.txt$, 1) as int) > 0)) AND ((regexp_extract(file_path#14, .*/(neg|pos|neu)/.*, 1) = pos) AND (length(value#12) > 100))))\n",
      "               +- Project [value#12, input_file_name() AS file_path#14]\n",
      "                  +- FileScan text [value#12] Batched: false, DataFilters: [], Format: Text, Location: InMemoryFileIndex(131669 paths)[file:/home/shoose/Документы/reviews/archive/dataset/neg/1000083-0..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "\n",
      "\n",
      "\n",
      "Сравнение производительности\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 100:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame API время: 22.388 сек\n",
      "Spark SQL время: 22.219 сек\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# cоздаём сложный запрос\n",
    "complex_query = df_cleaned \\\n",
    "    .filter(col(\"sentiment\") == \"pos\") \\\n",
    "    .filter(col(\"review_length\") > 100) \\\n",
    "    .groupBy(\"sentiment\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"count\"),\n",
    "        avg(\"review_length\").alias(\"avg_len\"),\n",
    "        expr(\"percentile_approx(review_length, 0.9)\").alias(\"p90_len\")\n",
    "    )\n",
    "\n",
    "print(\"План выполнения запроса\")\n",
    "print(\"\\nФорматированный план:\")\n",
    "complex_query.explain(\"formatted\")\n",
    "\n",
    "print(\"\\nСравнение с SQL\")\n",
    "df_cleaned.createOrReplaceTempView(\"cleaned_reviews\")\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT sentiment,\n",
    "       COUNT(*) as count,\n",
    "       AVG(review_length) as avg_len,\n",
    "       PERCENTILE_APPROX(review_length, 0.9) as p90_len\n",
    "FROM cleaned_reviews\n",
    "WHERE sentiment = 'pos' AND review_length > 100\n",
    "GROUP BY sentiment\n",
    "\"\"\"\n",
    "\n",
    "sql_result = spark.sql(sql_query)\n",
    "print(\"SQL план выполнения:\")\n",
    "sql_result.explain()\n",
    "\n",
    "# cравним производительность\n",
    "\n",
    "print(\"\\nСравнение производительности\")\n",
    "\n",
    "start = time.time()\n",
    "df_result = complex_query.collect()\n",
    "df_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "sql_result.collect()\n",
    "sql_time = time.time() - start\n",
    "\n",
    "print(f\"DataFrame API время: {df_time:.3f} сек\")\n",
    "print(f\"Spark SQL время: {sql_time:.3f} сек\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e585d132",
   "metadata": {},
   "source": [
    "\n",
    "## Как Catalyst оптимизирует НАШ конкретный запрос\n",
    "\n",
    "### Наш запрос из практики:\n",
    "```python\n",
    "complex_query = df_cleaned \\\n",
    "    .filter(col(\"sentiment\") == \"pos\") \\\n",
    "    .filter(col(\"review_length\") > 100) \\\n",
    "    .groupBy(\"sentiment\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"count\"),\n",
    "        avg(\"review_length\").alias(\"avg_len\"),\n",
    "        expr(\"percentile_approx(review_length, 0.9)\").alias(\"p90_len\")\n",
    "    )\n",
    "```\n",
    "\n",
    "### Обоснование оптимизаций Catalyst:\n",
    "\n",
    "1. Filter Merging (объединение фильтров):\n",
    "```python\n",
    "# Что мы написали:\n",
    ".filter(col(\"sentiment\") == \"pos\") \\\n",
    ".filter(col(\"review_length\") > 100) \\\n",
    "\n",
    "# Что делает Catalyst:\n",
    ".filter((col(\"sentiment\") == \"pos\") & (col(\"review_length\") > 100))\n",
    "```\n",
    "Почему это важно: Вместо двух проходов по данным - один проход с объединенным условием.\n",
    "\n",
    "2. Predicate Pushdown (проталкивание предикатов):\n",
    "```\n",
    "Если наши данные хранятся в Parquet (а мы их туда запишем),\n",
    "Catalyst \"проталкивает\" фильтры в чтение данных:\n",
    "\n",
    "ЧТЕНИЕ БЕЗ оптимизации:\n",
    "1. Читаем ВСЕ данные из Parquet\n",
    "2. Фильтруем в памяти\n",
    "\n",
    "ЧТЕНИЕ С оптимизацией:\n",
    "1. Читаем ТОЛЬКО данные где sentiment='pos' и review_length>100\n",
    "2. Уже отфильтрованные данные попадают в память\n",
    "```\n",
    "\n",
    "3. Projection Pruning (отсечение колонок):\n",
    "```python\n",
    "# Наш запрос использует только:\n",
    "# - sentiment (для фильтра и группировки)\n",
    "# - review_length (для фильтра и агрегаций)\n",
    "\n",
    "# Catalyst понимает, что НЕ нужны:\n",
    "# - review_text (большой текст)\n",
    "# - file_path (путь к файлу)\n",
    "# - film_id, review_num (метаданные)\n",
    "\n",
    "# Результат: читаем только 2 колонки вместо 6!\n",
    "```\n",
    "\n",
    "### Визуализация оптимизации для нашего запроса:\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Наш запрос<br/>filter→filter→groupBy→agg] --> B\n",
    "    \n",
    "    subgraph B [Logical Plan до оптимизации]\n",
    "        B1[Project: sentiment, count, avg_len, p90_len]\n",
    "        B2[HashAggregate: GROUP BY sentiment]\n",
    "        B3[Filter: review_length > 100]\n",
    "        B4[Filter: sentiment = 'pos']\n",
    "        B5[Scan: ВСЕ 6 колонок]\n",
    "        \n",
    "        B1 --> B2 --> B3 --> B4 --> B5\n",
    "    end\n",
    "    \n",
    "    A --> C\n",
    "    \n",
    "    subgraph C [Optimized Logical Plan после Catalyst]\n",
    "        C1[Project: sentiment, count, avg_len, p90_len]\n",
    "        C2[HashAggregate: GROUP BY sentiment]\n",
    "        C3[Filter: sentiment='pos' AND review_length>100]\n",
    "        C4[Scan: ТОЛЬКО sentiment, review_length]\n",
    "        \n",
    "        C1 --> C2 --> C3 --> C4\n",
    "    end\n",
    "    \n",
    "    style B fill:#ffebee\n",
    "    style C fill:#e8f5e9\n",
    "```\n",
    "\n",
    "### Числовая выгода для нашего ETL:\n",
    "\n",
    "```\n",
    "РАСЧЕТ ЭКОНОМИИ:\n",
    "\n",
    "БЕЗ оптимизации:\n",
    "• Чтение: 6 колонок × 131000 записей\n",
    "• Фильтрация: 2 прохода по данным\n",
    "• Память: все колонки в памяти\n",
    "\n",
    "С оптимизацией Catalyst:\n",
    "• Чтение: 2 колонки × ~8000 записей (после фильтрации)\n",
    "• Фильтрация: 1 проход по данным\n",
    "• Память: только нужные колонки\n",
    "\n",
    "ЭКОНОМИЯ:\n",
    "• Данные для чтения: в 3 раза меньше (6→2 колонок)\n",
    "• Данные в памяти: в 3 раза меньше\n",
    "• Операции фильтрации: в 2 раза меньше\n",
    "```\n",
    "\n",
    "### Почему SQL дает тот же план выполнения:\n",
    "\n",
    "```python\n",
    "# DataFrame API\n",
    "complex_query = df_cleaned.filter(col(\"sentiment\") == \"pos\")...\n",
    "\n",
    "# Spark SQL\n",
    "sql_query = \"SELECT ... WHERE sentiment = 'pos' ...\"\n",
    "\n",
    "# Catalyst создает ОДИН Abstract Syntax Tree (AST) для обоих!\n",
    "```\n",
    "\n",
    "Ключевой вывод: Независимо от того, пишем ли мы на DataFrame API или SQL, Catalyst применяет одинаковые оптимизации. Это делает Spark декларативным движком - мы говорим \"что\" нужно сделать, а Spark решает \"как\" это сделать оптимально.\n",
    "Это и есть сила Catalyst Optimizer - он позволяет писать чистый, понятный код, не жертвуя производительностью."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b5def4",
   "metadata": {},
   "source": [
    "## 8. Оптимизированная запись данных\n",
    "\n",
    "## 8.1. Детальное сравнение форматов (таблица)\n",
    "\n",
    "| Параметр | CSV | JSON | Parquet | ORC | Avro |\n",
    "|-------------|---------|----------|-------------|---------|----------|\n",
    "| Тип формата | Текстовый | Текстовый | Бинарный, колоночный | Бинарный, колоночный | Бинарный, строчный |\n",
    "| Сжатие | Обычно нет | Обычно нет | Отличное (3-5x) | Отличное (3-5x) | Хорошее (2-3x) |\n",
    "| Чтение | Медленное | Медленное | Быстрое (predicate pushdown) | Быстрое | Среднее |\n",
    "| Запись | Медленная | Медленная | Средняя | Средняя | Быстрая |\n",
    "| Схема | Нет (все строки) | Полуструктурированный | Строгая (типы данных) | Строгая | Schema evolution |\n",
    "| Поддержка в Spark | Полная | Полная | Нативная оптимизация | Хорошая | Хорошая |\n",
    "| Идеальный случай | Маленькие данные, обмен | Веб-API, полуструктурированные | Аналитика, большие данные | Hive, аналитика | Потоковая обработка |\n",
    "\n",
    "### Для нашего ETL с отзывами:\n",
    "- CSV: 160MB → ~200MB (без сжатия)\n",
    "- Parquet: 160MB → ~40MB (сжатие 4x) \n",
    "- Чтение только негативных отзывов: \n",
    "  - CSV: читаем 200MB, фильтруем в памяти\n",
    "  - Parquet: читаем ~13MB (только папку sentiment=neg/)\n",
    "\n",
    "---\n",
    "\n",
    "## 8.2. Алгоритмы сжатия в Parquet\n",
    "\n",
    "| Алгоритм | Скорость | Сжатие | CPU Usage | Рекомендация |\n",
    "|-------------|-------------|------------|---------------|------------------|\n",
    "| uncompressed | Быстрее всего | Нет сжатия | Низкий | Только для тестирования |\n",
    "| snappy | Быстрое | Хорошее (2-3x) | Низкий | По умолчанию, баланс скорости/сжатия |\n",
    "| gzip | Медленное | Отличное (4-5x) | Высокий | Для архивных данных |\n",
    "| lzo | Быстрое | Среднее | Низкий | Для Hadoop экосистемы |\n",
    "| zstd | Средняя | Отличное (4-5x) | Средний | Современная альтернатива gzip |\n",
    "\n",
    "Для нашего ETL: `compression=\"snappy\"` - оптимальный баланс.\n",
    "\n",
    "---\n",
    "\n",
    "## 8.3. Стратегии партиционирования\n",
    "\n",
    "| Стратегия | Преимущества | Недостатки | Когда использовать |\n",
    "|--------------|-----------------|----------------|------------------------|\n",
    "| По тональности (sentiment) | Быстрые фильтры по sentiment | 3 папки, макс 3 ускорение | ✅ НАШ СЛУЧАЙ - частые фильтры по sentiment |\n",
    "| По film_id | Быстрые запросы по фильму | Слишком много папок (1000+) | Если часто ищут по конкретным фильмам |\n",
    "| По дате (год/месяц) | Временные анализы | Нужна колонка даты | Для временных рядов |\n",
    "| Bucketing (по film_id) | Оптимизация JOIN | Сложная настройка | Для частых JOIN с таблицей фильмов |\n",
    "| Комбинированное (sentiment + год) | Максимальная гибкость | Сложное управление | Для production систем |\n",
    "\n",
    "---\n",
    "\n",
    "## 8.4. Расчет оптимального количества файлов (таблица)\n",
    "\n",
    "| Размер данных | Рекомендуемое кол-во файлов | Размер файла | Пример для наших данных |\n",
    "|------------------|--------------------------------|------------------|----------------------------|\n",
    "| < 1GB | 2-8 файлов | 64-512MB | 160MB → 4 файла по ~40MB |\n",
    "| 1-10GB | 10-100 файлов | 100MB-1GB | - |\n",
    "| 10-100GB | 100-500 файлов | 100MB-200MB | - |\n",
    "| > 100GB | 500-2000 файлов | 100MB-200MB | - |\n",
    "\n",
    "Формула расчета:\n",
    "```\n",
    "Оптимальное_кол-во_файлов = ceil(Общий_размер / 128MB)\n",
    "Для наших данных: ceil(160MB / 128MB) = 2 файла\n",
    "С партиционированием: 3 категории × 2 файла = 6 файлов\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8.5. Сравнение методов записи (таблица)\n",
    "\n",
    "| Метод записи | Код | Преимущества | Недостатки | Производительность |\n",
    "|-----------------|---------|------------------|----------------|------------------------|\n",
    "| Базовая | `.write.parquet()` | Простота | Мелкие файлы, нет оптимизаций | Средняя |\n",
    "| С партиционированием | `.partitionBy().parquet()` | Быстрые фильтры | Overhead папок | Высокая для фильтрации |\n",
    "| С контролем файлов | `.repartition().write()` | Оптимальный размер | Дополнительный shuffle | Высокая |\n",
    "| С динамическим | `.writeBucketed()` | Оптимизация JOIN | Сложная настройка | Очень высокая для JOIN |\n",
    "| С инкрементальной | `.mode(\"append\")` | Добавление данных | Риск дубликатов | Зависит от объема |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99a1355",
   "metadata": {},
   "source": [
    "#### Оптимизированная запись данных\n",
    "\n",
    "Сохранить данные в оптимальном формате для дальнейшего использования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b317fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные сохранены в форматах:\n",
      "1. CSV: ./data/processed/reviews_csv\n",
      "2. Parquet: ./data/processed/reviews.parquet\n",
      "3. Parquet с партиционированием: ./data/processed/reviews_partitioned\n",
      "\n",
      "Сравнение размеров\n",
      "CSV: 0.00 MB\n",
      "Parquet: 0.00 MB\n",
      "Parquet partitioned: 0.00 MB\n",
      "\n",
      "Преимущества партиционирования\n",
      "Чтение только негативных отзывов из партиционированных данных:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 109:===================================================> (120 + 3) / 123]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Партиционированные: 1.798 сек, 16697 записей\n",
      "Непартиционированные: 1.812 сек, 16697 записей\n",
      "Выигрыш: 1.0x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "output_dir = \"./data/processed\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# CSV (для совместимости)\n",
    "csv_path = f\"{output_dir}/reviews_csv\"\n",
    "df_cleaned.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(csv_path)\n",
    "\n",
    "# Parquet (оптимизированный бинарный формат)\n",
    "parquet_path = f\"{output_dir}/reviews.parquet\"\n",
    "df_cleaned.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(parquet_path)\n",
    "\n",
    "# Parquet с партиционированием\n",
    "partitioned_path = f\"{output_dir}/reviews_partitioned\"\n",
    "df_cleaned.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"sentiment\") \\\n",
    "    .parquet(partitioned_path)\n",
    "\n",
    "print(\"Данные сохранены в форматах:\")\n",
    "print(f\"1. CSV: {csv_path}\")\n",
    "print(f\"2. Parquet: {parquet_path}\")\n",
    "print(f\"3. Parquet с партиционированием: {partitioned_path}\")\n",
    "\n",
    "print(\"\\nСравнение размеров\")\n",
    "for format_name, path in [(\"CSV\", csv_path), (\"Parquet\", parquet_path), (\"Parquet partitioned\", partitioned_path)]:\n",
    "    total_size = 0\n",
    "    for file in glob.glob(f\"{path}/\", recursive=True):\n",
    "        if os.path.isfile(file):\n",
    "            total_size += os.path.getsize(file)\n",
    "    print(f\"{format_name}: {total_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "print(\"\\nПреимущества партиционирования\")\n",
    "print(\"Чтение только негативных отзывов из партиционированных данных:\")\n",
    "\n",
    "start = time.time()\n",
    "neg_reviews = spark.read.parquet(f\"{partitioned_path}/sentiment=neg\")\n",
    "neg_count = neg_reviews.count()\n",
    "partitioned_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "neg_reviews2 = spark.read.parquet(parquet_path).filter(col(\"sentiment\") == \"neg\")\n",
    "neg_count2 = neg_reviews2.count()\n",
    "non_partitioned_time = time.time() - start\n",
    "\n",
    "print(f\"Партиционированные: {partitioned_time:.3f} сек, {neg_count} записей\")\n",
    "print(f\"Непартиционированные: {non_partitioned_time:.3f} сек, {neg_count2} записей\")\n",
    "print(f\"Выигрыш: {non_partitioned_time/partitioned_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ea71ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Данные сохранены в 3 форматах\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Размер файлов:\n",
      "  CSV: 402.10 MB\n",
      "  Parquet: 221.77 MB\n",
      "  Parquet с партиц.: 244.22 MB\n",
      "\n",
      " Тест скорости чтения:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 123:====================================>                (84 + 12) / 123]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Партиционированные: 1.09 сек\n",
      "  Непартиционированные: 1.38 сек\n",
      "  Ускорение: 1.3 раз\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "output_dir = \"./data/processed\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# запись данных в разные форматы\n",
    "csv_path = f\"{output_dir}/reviews_csv\"\n",
    "parquet_path = f\"{output_dir}/reviews.parquet\"\n",
    "partitioned_path = f\"{output_dir}/reviews_partitioned\"\n",
    "\n",
    "# записываем все форматы\n",
    "df_cleaned.write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_path)\n",
    "df_cleaned.write.mode(\"overwrite\").parquet(parquet_path)\n",
    "df_cleaned.write.mode(\"overwrite\").partitionBy(\"sentiment\").parquet(partitioned_path)\n",
    "\n",
    "print(\"✓ Данные сохранены в 3 форматах\")\n",
    "\n",
    "# Spark ленивый - нужно вызвать действие, чтобы запись реально произошла\n",
    "df_cleaned.count()  # это заставит Spark выполнить все pending операции\n",
    "time.sleep(2)  # ждем завершения записи\n",
    "\n",
    "def get_size(path):\n",
    "    if not os.path.exists(path):\n",
    "        return 0\n",
    "    total = 0\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            filepath = os.path.join(root, file)\n",
    "            if os.path.isfile(filepath):\n",
    "                total += os.path.getsize(filepath)\n",
    "    return total / (1024*1024)  # в MB\n",
    "\n",
    "print(\"\\n Размер файлов:\")\n",
    "sizes = []\n",
    "for name, path in [(\"CSV\", csv_path), (\"Parquet\", parquet_path), (\"Parquet с партиц.\", partitioned_path)]:\n",
    "    size_mb = get_size(path)\n",
    "    sizes.append((name, size_mb))\n",
    "    print(f\"  {name}: {size_mb:.2f} MB\")\n",
    "\n",
    "print(\"\\n Тест скорости чтения:\")\n",
    "\n",
    "start = time.time()\n",
    "neg1 = spark.read.parquet(f\"{partitioned_path}/sentiment=neg\")\n",
    "count1 = neg1.count()\n",
    "time1 = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "neg2 = spark.read.parquet(parquet_path).filter(col(\"sentiment\") == \"neg\")\n",
    "count2 = neg2.count()\n",
    "time2 = time.time() - start\n",
    "\n",
    "print(f\"  Партиционированные: {time1:.2f} сек\")\n",
    "print(f\"  Непартиционированные: {time2:.2f} сек\")\n",
    "print(f\"  Ускорение: {time2/time1:.1f} раз\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558d5d7a",
   "metadata": {},
   "source": [
    "## 9. Создание итогового отчета\n",
    "\n",
    "### 9.1. Метрики качества ETL-пайплайна\n",
    "\n",
    "```\n",
    "Основные метрики:\n",
    "1. Input/Output: Сколько данных прочитано/записано\n",
    "2. Время выполнения: Общее время пайплайна\n",
    "3. Обработка ошибок: Сколько файлов пропущено\n",
    "4. Распределение данных: Баланс по категориям\n",
    "5. Размеры файлов: Оптимальность хранения\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bbd423",
   "metadata": {},
   "source": [
    "#### Создание итогового отчёта\n",
    "\n",
    "Создать сводный отчёт по проделанной работе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c79f7978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Подготовка метрик для отчета...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего обработано файлов                           131669\n",
      "После очистки                                     109998\n",
      "Удалено записей                                    21671\n",
      "Процент удаленных                                  16.46%\n",
      "Негативных отзывов                                 16697\n",
      "Позитивных отзывов                                 72987\n",
      "Нейтральных отзывов                                20314\n",
      "Уникальных фильмов (film_id)                        9065\n",
      "Среднее отзывов на фильм                           12.13\n",
      "Максимум отзывов на фильм                            104\n",
      "Средний номер рецензии                             27.26\n",
      "Минимальный review_num                                 1\n",
      "Максимальный review_num                               99\n",
      "Средняя длина отзыва                             2042.84\n",
      "Медианная длина отзыва                              1845\n",
      "Стандартное отклонение длины                     1054.62\n",
      "Минимальная длина                                     58\n",
      "Максимальная длина                                  5000\n",
      "Среднее слов в отзыве                             304.56\n",
      "Дубликатов film_id+review_num                         68\n",
      "Некорректных film_id                                   0\n",
      "Некорректных review_num                            15613\n",
      "Корреляция тональность-длина                      -0.012\n",
      "Корреляция длина-слова                             0.994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Распределение по тональности:\n",
      "  Негативные:   15.2% (16697 отзывов)\n",
      "  Позитивные:   66.4% (72987 отзывов)\n",
      "  Нейтральные:   18.5% (20314 отзывов)\n",
      "\n",
      "Топ-5 фильмов по количеству отзывов:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1. Фильм ID 1048334: 104 отзывов\n",
      "  2. Фильм ID 493768: 99 отзывов\n",
      "  3. Фильм ID 2950: 99 отзывов\n",
      "  4. Фильм ID 481086: 99 отзывов\n",
      "  5. Фильм ID 8219: 99 отзывов\n",
      "\n",
      "Анализ длины отзывов:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Q1 (25-й перцентиль): 1240 символов\n",
      "  Q3 (75-й перцентиль): 2688 символов\n",
      "  IQR: 1448 символов\n",
      "  Границы выбросов: -932 - 4860\n",
      "Сохранение отчета в файл: ./data/processed/etl_report_20251219_155258.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Отчет успешно сохранен в файл: ./data/processed/etl_report_20251219_155258.txt\n",
      "Общее время выполнения ETL: 815.95 секунд\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработано отзывов в секунду: 134.8\n",
      "\n",
      "ETL пайплайн успешно завершен!\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.functions import countDistinct, stddev, expr, corr, when\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# директория для результатов \n",
    "output_dir = \"./data/processed\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Подготовка метрик для отчета...\")\n",
    "\n",
    "# подсчет дубликатов\n",
    "duplicate_count = df_cleaned.groupBy(\"film_id\", \"review_num\") \\\n",
    "    .count() \\\n",
    "    .filter(col(\"count\") > 1) \\\n",
    "    .count()\n",
    "\n",
    "# статистика по фильмам\n",
    "film_stats = df_cleaned.groupBy(\"film_id\") \\\n",
    "    .count() \\\n",
    "    .agg(\n",
    "        avg(\"count\").alias(\"avg_reviews_per_film\"),\n",
    "        max(\"count\").alias(\"max_reviews_per_film\")\n",
    "    ).first()\n",
    "\n",
    "# корреляции\n",
    "df_numeric = df_cleaned.withColumn(\n",
    "    \"sentiment_score\",\n",
    "    when(col(\"sentiment\") == \"neg\", -1)\n",
    "    .when(col(\"sentiment\") == \"neu\", 0)\n",
    "    .when(col(\"sentiment\") == \"pos\", 1)\n",
    "    .otherwise(None)\n",
    ")\n",
    "\n",
    "corr_data = df_numeric.select(\n",
    "    corr(\"sentiment_score\", \"review_length\").alias(\"corr_sentiment_length\"),\n",
    "    corr(\"review_length\", \"word_count\").alias(\"corr_length_words\")\n",
    ").first()\n",
    "\n",
    "# формирование данных отчета\n",
    "report_data = [\n",
    "    # базовые метрики обработки\n",
    "    (\"Всего обработано файлов\", df.count()),\n",
    "    (\"После очистки\", df_cleaned.count()),\n",
    "    (\"Удалено записей\", df.count() - df_cleaned.count()),\n",
    "    (\"Процент удаленных\", ((df.count() - df_cleaned.count()) / df.count() * 100) if df.count() > 0 else 0),\n",
    "    \n",
    "    # распределение по тональности\n",
    "    (\"Негативных отзывов\", df_cleaned.filter(col(\"sentiment\") == \"neg\").count()),\n",
    "    (\"Позитивных отзывов\", df_cleaned.filter(col(\"sentiment\") == \"pos\").count()),\n",
    "    (\"Нейтральных отзывов\", df_cleaned.filter(col(\"sentiment\") == \"neu\").count()),\n",
    "    \n",
    "    # метаданные фильмов\n",
    "    (\"Уникальных фильмов (film_id)\", df_cleaned.select(countDistinct(\"film_id\")).first()[0]),\n",
    "    (\"Среднее отзывов на фильм\", film_stats[\"avg_reviews_per_film\"]),\n",
    "    (\"Максимум отзывов на фильм\", film_stats[\"max_reviews_per_film\"]),\n",
    "    \n",
    "    # статистика review_num\n",
    "    (\"Средний номер рецензии\", df_cleaned.select(avg(\"review_num\")).first()[0]),\n",
    "    (\"Минимальный review_num\", df_cleaned.select(min(\"review_num\")).first()[0]),\n",
    "    (\"Максимальный review_num\", df_cleaned.select(max(\"review_num\")).first()[0]),\n",
    "    \n",
    "    # статистика длины текста\n",
    "    (\"Средняя длина отзыва\", df_cleaned.select(avg(\"review_length\")).first()[0]),\n",
    "    (\"Медианная длина отзыва\", df_cleaned.select(expr(\"percentile_approx(review_length, 0.5)\")).first()[0]),\n",
    "    (\"Стандартное отклонение длины\", df_cleaned.select(stddev(\"review_length\")).first()[0]),\n",
    "    (\"Минимальная длина\", df_cleaned.select(min(\"review_length\")).first()[0]),\n",
    "    (\"Максимальная длина\", df_cleaned.select(max(\"review_length\")).first()[0]),\n",
    "    \n",
    "    # статистика по словам\n",
    "    (\"Среднее слов в отзыве\", df_cleaned.select(avg(\"word_count\")).first()[0]),\n",
    "    \n",
    "    # качество данных\n",
    "    (\"Дубликатов film_id+review_num\", duplicate_count),\n",
    "    (\"Некорректных film_id\", df.filter((col(\"film_id\").isNull()) | (col(\"film_id\") <= 0)).count()),\n",
    "    (\"Некорректных review_num\", df.filter((col(\"review_num\").isNull()) | (col(\"review_num\") <= 0)).count()),\n",
    "    \n",
    "    # корреляции\n",
    "    (\"Корреляция тональность-длина\", corr_data[\"corr_sentiment_length\"]),\n",
    "    (\"Корреляция длина-слова\", corr_data[\"corr_length_words\"]),\n",
    "]\n",
    "\n",
    "\n",
    "for metric, value in report_data:\n",
    "    if isinstance(value, float):\n",
    "        if metric.startswith(\"Корреляция\"):\n",
    "            print(f\"{metric:40} {value:15.3f}\")\n",
    "        elif metric.startswith(\"Процент\"):\n",
    "            print(f\"{metric:40} {value:15.2f}%\")\n",
    "        else:\n",
    "            print(f\"{metric:40} {value:15.2f}\")\n",
    "    else:\n",
    "        print(f\"{metric:40} {value:15d}\")\n",
    "\n",
    "\n",
    "# распределение по тональности в процентах\n",
    "total_clean = df_cleaned.count()\n",
    "if total_clean > 0:\n",
    "    neg_count = df_cleaned.filter(col(\"sentiment\") == \"neg\").count()\n",
    "    pos_count = df_cleaned.filter(col(\"sentiment\") == \"pos\").count()\n",
    "    neu_count = df_cleaned.filter(col(\"sentiment\") == \"neu\").count()\n",
    "    \n",
    "    neg_pct = neg_count / total_clean * 100\n",
    "    pos_pct = pos_count / total_clean * 100\n",
    "    neu_pct = neu_count / total_clean * 100\n",
    "    \n",
    "    print(f\"\\nРаспределение по тональности:\")\n",
    "    print(f\"  Негативные: {neg_pct:6.1f}% ({neg_count} отзывов)\")\n",
    "    print(f\"  Позитивные: {pos_pct:6.1f}% ({pos_count} отзывов)\")\n",
    "    print(f\"  Нейтральные: {neu_pct:6.1f}% ({neu_count} отзывов)\")\n",
    "\n",
    "# топ-5 фильмов по количеству отзывов\n",
    "print(f\"\\nТоп-5 фильмов по количеству отзывов:\")\n",
    "top_films = df_cleaned.groupBy(\"film_id\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .limit(5) \\\n",
    "    .collect()\n",
    "\n",
    "for i, row in enumerate(top_films, 1):\n",
    "    print(f\"  {i}. Фильм ID {row['film_id']}: {row['count']} отзывов\")\n",
    "\n",
    "# анализ длины отзывов\n",
    "print(f\"\\nАнализ длины отзывов:\")\n",
    "length_stats = df_cleaned.select(\n",
    "    expr(\"percentile_approx(review_length, 0.25)\").alias(\"q1\"),\n",
    "    expr(\"percentile_approx(review_length, 0.75)\").alias(\"q3\")\n",
    ").first()\n",
    "\n",
    "iqr = length_stats[\"q3\"] - length_stats[\"q1\"]\n",
    "lower_bound = length_stats[\"q1\"] - 1.5 * iqr\n",
    "upper_bound = length_stats[\"q3\"] + 1.5 * iqr\n",
    "\n",
    "print(f\"  Q1 (25-й перцентиль): {length_stats['q1']:.0f} символов\")\n",
    "print(f\"  Q3 (75-й перцентиль): {length_stats['q3']:.0f} символов\")\n",
    "print(f\"  IQR: {iqr:.0f} символов\")\n",
    "print(f\"  Границы выбросов: {lower_bound:.0f} - {upper_bound:.0f}\")\n",
    "\n",
    "# сохранение отчета в файл\n",
    "report_filename = f\"{output_dir}/etl_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "print(f\"Сохранение отчета в файл: {report_filename}\")\n",
    "\n",
    "with open(report_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Основные метрики обработки:\\n\")\n",
    "    for metric, value in report_data:\n",
    "        if isinstance(value, float):\n",
    "            if metric.startswith(\"Корреляция\"):\n",
    "                f.write(f\"{metric}: {value:.3f}\\n\")\n",
    "            elif metric.startswith(\"Процент\"):\n",
    "                f.write(f\"{metric}: {value:.2f}%\\n\")\n",
    "            else:\n",
    "                f.write(f\"{metric}: {value:.2f}\\n\")\n",
    "        else:\n",
    "            f.write(f\"{metric}: {value}\\n\")\n",
    "    \n",
    "    f.write(\"\\nАналитика данных:\\n\")\n",
    "    \n",
    "    if total_clean > 0:\n",
    "        f.write(f\"Распределение по тональности:\\n\")\n",
    "        f.write(f\"  • Негативные: {neg_pct:.1f}% ({neg_count} отзывов)\\n\")\n",
    "        f.write(f\"  • Позитивные: {pos_pct:.1f}% ({pos_count} отзывов)\\n\")\n",
    "        f.write(f\"  • Нейтральные: {neu_pct:.1f}% ({neu_count} отзывов)\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Топ-5 фильмов по количеству отзывов:\\n\")\n",
    "    for i, row in enumerate(top_films, 1):\n",
    "        f.write(f\"  {i}. Фильм ID {row['film_id']}: {row['count']} отзывов\\n\")\n",
    "    \n",
    "    f.write(f\"\\nСтатистические границы (IQR метод):\\n\")\n",
    "    f.write(f\"  • Q1 (25-й перцентиль): {length_stats['q1']:.0f} символов\\n\")\n",
    "    f.write(f\"  • Q3 (75-й перцентиль): {length_stats['q3']:.0f} символов\\n\")\n",
    "    f.write(f\"  • IQR: {iqr:.0f} символов\\n\")\n",
    "    f.write(f\"  • Границы выбросов: {lower_bound:.0f} - {upper_bound:.0f}\\n\")\n",
    "\n",
    "    f.write(f\"Сводка:\\n\")\n",
    "    f.write(f\"  • Обработано файлов: {df.count()}\\n\")\n",
    "    f.write(f\"  • Успешно очищено: {df_cleaned.count()} ({df_cleaned.count()/df.count()*100:.1f}%)\\n\")\n",
    "    f.write(f\"  • Уникальных фильмов: {df_cleaned.select(countDistinct('film_id')).first()[0]}\\n\")\n",
    "    f.write(f\"  • Средняя длина отзыва: {df_cleaned.select(avg('review_length')).first()[0]:.0f} символов\\n\")\n",
    "\n",
    "    f.write(\"Метаданные отчета:\\n\")\n",
    "    f.write(f\"  • Дата генерации: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"  • Spark Application: {spark.conf.get('spark.app.name')}\\n\")\n",
    "    f.write(f\"  • Режим выполнения: {spark.conf.get('spark.master')}\\n\")\n",
    "\n",
    "print(f\"Отчет успешно сохранен в файл: {report_filename}\")\n",
    "\n",
    "# статистика времени выполнения\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "print(f\"Общее время выполнения ETL: {total_time:.2f} секунд\")\n",
    "print(f\"Обработано отзывов в секунду: {df_cleaned.count()/total_time:.1f}\")\n",
    "\n",
    "spark.stop()\n",
    "print(\"\\nETL пайплайн успешно завершен!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
